{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79dbc81f",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "827d733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tf-keras\n",
    "%pip install -q transformers\n",
    "%pip install -q datasets\n",
    "%pip install -q nltk\n",
    "%pip install -q scikit-learn\n",
    "%pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377243ac",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab3d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress TensorFlow deprecation warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c94de",
   "metadata": {},
   "source": [
    "## 3. Load Dataset from JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f345f010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab drive mount failed: mount failed. Continuing to local search.\n",
      "\n",
      "Dataset resolution diagnostics:\n",
      "Current working directory: /content\n",
      "Home directory: /root\n",
      "Searched roots: 4\n",
      " - /\n",
      " - /content\n",
      " - /root\n",
      " - /tmp\n",
      "\n",
      "Dataset resolution diagnostics:\n",
      "Current working directory: /content\n",
      "Home directory: /root\n",
      "Searched roots: 4\n",
      " - /\n",
      " - /content\n",
      " - /root\n",
      " - /tmp\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not mount Google Drive or locate dataset folder. If running locally, set the environment variable GDRIVE_DATASET_PATH to the absolute path of your dataset folder, e.g., GDRIVE_DATASET_PATH=/Users/yourname/path/to/ml/dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3529360381.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Resolve dataset directory (will attempt to mount Drive on Colab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mdataset_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gdrive_dataset_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Load training, validation, and test datasets from the resolved directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3529360381.py\u001b[0m in \u001b[0;36mget_gdrive_dataset_dir\u001b[0;34m(subpath)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" - {root}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     raise RuntimeError(\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;34m\"Could not mount Google Drive or locate dataset folder. If running locally, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;34m\"set the environment variable GDRIVE_DATASET_PATH to the absolute path of your dataset folder, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not mount Google Drive or locate dataset folder. If running locally, set the environment variable GDRIVE_DATASET_PATH to the absolute path of your dataset folder, e.g., GDRIVE_DATASET_PATH=/Users/yourname/path/to/ml/dataset"
     ]
    }
   ],
   "source": [
    "def get_gdrive_dataset_dir(subpath='dataset'):\n",
    "    \"\"\"Locate the dataset folder, independent of working directory.\n",
    "    \n",
    "    Strategy (in order):\n",
    "    1. Try to mount Google Drive (Colab only).\n",
    "    2. Check GDRIVE_DATASET_PATH environment variable.\n",
    "    3. Search upward from common script/notebook directories (/tmp, user home, etc.).\n",
    "    4. Search the entire home directory tree (shallow).\n",
    "    5. Search system common locations.\n",
    "    \"\"\"\n",
    "    import pathlib\n",
    "    \n",
    "    # 1) Try Colab mount if available\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        try:\n",
    "            drive.mount('/content/drive', force_remount=False)\n",
    "            candidate = pathlib.Path('/content/drive/MyDrive') / subpath\n",
    "            if candidate.exists():\n",
    "                print(f\"Using dataset folder (Colab): {candidate}\")\n",
    "                return str(candidate)\n",
    "            print(f\"Colab mounted but {candidate} not found, continuing to local search.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Colab drive mount failed: {e}. Continuing to local search.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # 2) Environment variable\n",
    "    env_path = os.environ.get('GDRIVE_DATASET_PATH')\n",
    "    if env_path:\n",
    "        p = pathlib.Path(env_path)\n",
    "        if p.exists():\n",
    "            print(f\"Using dataset folder (env): {p}\")\n",
    "            return str(p.resolve())\n",
    "        print(f\"GDRIVE_DATASET_PATH set but does not exist: {env_path}\")\n",
    "    \n",
    "    # 3) Search from common notebook/script locations upward\n",
    "    search_roots = [\n",
    "        pathlib.Path.home(),  # User home directory\n",
    "        pathlib.Path('/tmp'),  # Common temp folder\n",
    "        pathlib.Path.cwd(),  # Current working directory\n",
    "    ]\n",
    "    \n",
    "    # Also add parent directories of common locations (e.g., in case we're in /tmp/xxx)\n",
    "    for root in list(search_roots):\n",
    "        for _ in range(5):\n",
    "            if root.parent == root:  # Reached filesystem root\n",
    "                break\n",
    "            search_roots.append(root.parent)\n",
    "            root = root.parent\n",
    "    \n",
    "    # Deduplicate\n",
    "    search_roots = list(set(search_roots))\n",
    "    \n",
    "    # 4) Search each root with limited depth\n",
    "    for root in search_roots:\n",
    "        try:\n",
    "            for dirpath in root.rglob('*'):\n",
    "                if dirpath.is_dir() and dirpath.name == subpath:\n",
    "                    print(f\"Found dataset folder: {dirpath}\")\n",
    "                    return str(dirpath.resolve())\n",
    "                # Limit depth to avoid infinite search\n",
    "                if dirpath.relative_to(root).parts.__len__() > 8:\n",
    "                    break\n",
    "        except (PermissionError, OSError):\n",
    "            continue\n",
    "    \n",
    "    # Fallback: print what we searched and raise\n",
    "    print(\"\\nDataset resolution diagnostics:\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Home directory: {pathlib.Path.home()}\")\n",
    "    print(f\"Searched roots: {len(search_roots)}\")\n",
    "    for root in sorted(search_roots)[:10]:\n",
    "        print(f\" - {root}\")\n",
    "    \n",
    "    raise RuntimeError(\n",
    "        \"Could not mount Google Drive or locate dataset folder. If running locally, \"\n",
    "        \"set the environment variable GDRIVE_DATASET_PATH to the absolute path of your dataset folder, \"\n",
    "        \"e.g., GDRIVE_DATASET_PATH=/Users/yourname/path/to/ml/dataset\"\n",
    "    )\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load data from JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Resolve dataset directory (will attempt to mount Drive on Colab)\n",
    "dataset_dir = get_gdrive_dataset_dir('dataset')\n",
    "\n",
    "# Load training, validation, and test datasets from the resolved directory\n",
    "train_data = load_jsonl(os.path.join(dataset_dir, 'train.jsonl'))\n",
    "\n",
    "val_data = load_jsonl(os.path.join(dataset_dir, 'val.jsonl'))\n",
    "\n",
    "test_data = load_jsonl(os.path.join(dataset_dir, 'test.jsonl'))\n",
    "\n",
    "print(f\"Dataset directory: {dataset_dir}\")\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Testing set size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63513a8e",
   "metadata": {},
   "source": [
    "## 4. Convert JSONL to DataFrame and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f880e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data:\n",
      "                                                text  label\n",
      "0  මත්ද්‍රව්‍ය ජාවාරමකට සම්බන්ධ පුද්ගලයෙකු පොලිස්...     AI\n",
      "1  ශ්‍රී ලංකාවේ නව කැබිනට් මණ්ඩලයේ සංශෝධනය පිළිබඳ...     AI\n",
      "2  2012 පෙබරවාරි මාසයේ වැල්ලම්පිටියේ දී යුද හමුදා...  HUMAN\n",
      "3  (මනෝප්‍රිය ගුණසේකර)කතෝලික දේවස්ථාන හා තරුපහේ හ...  HUMAN\n",
      "4  මැතිවරණ කොමිසම වෙත මැතිවරණ ආශ්‍රිත පැමිණිලි 45...     AI\n",
      "\n",
      "Label value counts (Train):\n",
      "label\n",
      "HUMAN    39848\n",
      "AI       32516\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Display sample data\n",
    "print(\"Sample training data:\")\n",
    "print(train_df[['text', 'label']].head())\n",
    "print(f\"\\nLabel value counts (Train):\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1d0e8",
   "metadata": {},
   "source": [
    "## 5. Map Labels to Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0634946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Unmapped labels: 0\n",
      "Val - Unmapped labels: 0\n",
      "Test - Unmapped labels: 0\n"
     ]
    }
   ],
   "source": [
    "# Create label mapping\n",
    "label_mapping = {'HUMAN': 0, 'AI': 1}\n",
    "reverse_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Map labels to numeric values\n",
    "train_df['label_encoded'] = train_df['label'].map(label_mapping)\n",
    "val_df['label_encoded'] = val_df['label'].map(label_mapping)\n",
    "test_df['label_encoded'] = test_df['label'].map(label_mapping)\n",
    "\n",
    "# Check for any unmapped values\n",
    "print(f\"Train - Unmapped labels: {train_df['label_encoded'].isna().sum()}\")\n",
    "print(f\"Val - Unmapped labels: {val_df['label_encoded'].isna().sum()}\")\n",
    "print(f\"Test - Unmapped labels: {test_df['label_encoded'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c185608",
   "metadata": {},
   "source": [
    "## 6. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bf614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Preprocessing validation data...\n",
      "Preprocessing validation data...\n",
      "Preprocessing test data...\n",
      "Preprocessing test data...\n",
      "Preprocessing complete!\n",
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"Expand common Sinhala contractions and perform Sinhala-specific preprocessing\"\"\"\n",
    "    # Sinhala contractions and abbreviations\n",
    "    contractions = {\n",
    "        \"ඔයා\": \"ඔබ\",  # colloquial to formal \"you\"\n",
    "        \"මට\": \"මට\",  # keep as is (already expanded)\n",
    "        \"තමා\": \"තමා\",  # keep as is\n",
    "        \"දෙයි\": \"දෙයි\",  # keep as is\n",
    "    }\n",
    "    \n",
    "    for contraction, expanded in contractions.items():\n",
    "        text = text.replace(contraction, expanded)\n",
    "    \n",
    "    # Remove extra whitespace and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove common Sinhala diacritics and combining marks if needed\n",
    "    text = re.sub(r'[\\u0981-\\u0983]', '', text)  # Remove combining marks\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text preprocessing\n",
    "print(\"Preprocessing training data...\")\n",
    "train_df['expanded_text'] = train_df['text'].apply(expand_contractions)\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_df['expanded_text'] = val_df['text'].apply(expand_contractions)\n",
    "\n",
    "print(\"Preprocessing test data...\")\n",
    "test_df['expanded_text'] = test_df['text'].apply(expand_contractions)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ce4ef",
   "metadata": {},
   "source": [
    "## 7. Load BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100265c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileBERT tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load MobileBERT tokenizer (very lightweight model, ~150MB, supports multilingual)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "print(\"MobileBERT tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d474d1d",
   "metadata": {},
   "source": [
    "## 8. Tokenize and Encode Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb99b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data in batches...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Tokenize and encode the text data using tokenizer in batches to avoid memory spikes\n",
    "print(\"Tokenizing data in batches...\")\n",
    "\n",
    "def tokenize_in_batches(texts, tokenizer, batch_size=32, max_length=512):\n",
    "    \"\"\"Tokenize a list of texts in small batches and return numpy arrays.\n",
    "    Returns a dict with 'input_ids' and 'attention_mask' as numpy arrays.\n",
    "    Pads every batch to `max_length` so concatenation shapes match.\n",
    "    \"\"\"\n",
    "    input_ids_parts = []\n",
    "    attention_mask_parts = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            padding='max_length',   # pad each batch to the fixed max_length\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        # Convert to numpy to keep memory usage predictable and avoid nested lists\n",
    "        ids = enc['input_ids'].numpy()\n",
    "        mask = enc['attention_mask'].numpy()\n",
    "\n",
    "        # Sanity check: ensure shape[1] == max_length\n",
    "        if ids.shape[1] != max_length:\n",
    "            # If tokenizer produced a different length for some reason, force-pad/truncate\n",
    "            ids = np.pad(ids, ((0,0),(0,max_length-ids.shape[1])), constant_values=0)[:,:max_length]\n",
    "            mask = np.pad(mask, ((0,0),(0,max_length-mask.shape[1])), constant_values=0)[:,:max_length]\n",
    "\n",
    "        input_ids_parts.append(ids)\n",
    "        attention_mask_parts.append(mask)\n",
    "\n",
    "    input_ids = np.concatenate(input_ids_parts, axis=0)\n",
    "    attention_mask = np.concatenate(attention_mask_parts, axis=0)\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "# Run batched tokenization\n",
    "train_encodings = tokenize_in_batches(train_df['expanded_text'].tolist(), tokenizer, batch_size=32)\n",
    "val_encodings = tokenize_in_batches(val_df['expanded_text'].tolist(), tokenizer, batch_size=32)\n",
    "test_encodings = tokenize_in_batches(test_df['expanded_text'].tolist(), tokenizer, batch_size=32)\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "print(f\"Training encodings shape: {train_encodings['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c7630",
   "metadata": {},
   "source": [
    "## 9. Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b2bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels shape: (72364,)\n",
      "Val labels shape: (9045,)\n",
      "Test labels shape: (9048,)\n",
      "\n",
      "Label distribution (Train): [39848 32516]\n",
      "Label distribution (Val): [4981 4064]\n",
      "Label distribution (Test): [4982 4066]\n"
     ]
    }
   ],
   "source": [
    "# Extract input arrays and convert labels to numpy arrays\n",
    "# Use dicts with both input_ids and attention_mask so the model gets both inputs\n",
    "train_inputs = {\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask']\n",
    "}\n",
    "val_inputs = {\n",
    "    'input_ids': val_encodings['input_ids'],\n",
    "    'attention_mask': val_encodings['attention_mask']\n",
    "}\n",
    "test_inputs = {\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask']\n",
    "}\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "train_labels = np.array(train_df['label_encoded'].astype(int).tolist())\n",
    "val_labels = np.array(val_df['label_encoded'].astype(int).tolist())\n",
    "test_labels = np.array(test_df['label_encoded'].astype(int).tolist())\n",
    "\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Val labels shape: {val_labels.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "print(f\"\\nLabel distribution (Train): {np.bincount(train_labels)}\")\n",
    "print(f\"Label distribution (Val): {np.bincount(val_labels)}\")\n",
    "print(f\"Label distribution (Test): {np.bincount(test_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4522d808",
   "metadata": {},
   "source": [
    "## 10. Define BERT Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "The paging file is too small for this operation to complete. (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load pre-trained DistilBERT model for sequence classification (lighter version)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TFDistilBertForSequenceClassification\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mTFDistilBertForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdistilbert-base-multilingual-cased\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Binary classification: HUMAN (0) vs AI (1)\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDistilBERT model loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pasindu Gunarathne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:2903\u001b[39m, in \u001b[36mTFPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   2901\u001b[39m safetensors_from_pt = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2902\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename == SAFE_WEIGHTS_NAME:\n\u001b[32m-> \u001b[39m\u001b[32m2903\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   2904\u001b[39m         safetensors_metadata = f.metadata()\n\u001b[32m   2905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m safetensors_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m safetensors_metadata.get(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mflax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmlx\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mOSError\u001b[39m: The paging file is too small for this operation to complete. (os error 1455)"
     ]
    }
   ],
   "source": [
    "# Load pre-trained MobileBERT model for sequence classification (very lightweight, ~150MB)\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    'google/mobilebert-uncased',\n",
    "    num_labels=2  # Binary classification: HUMAN (0) vs AI (1)\n",
    ")\n",
    "\n",
    "print(\"MobileBERT model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d250e44",
   "metadata": {},
   "source": [
    "## 11. Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[metric]\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112c1b3",
   "metadata": {},
   "source": [
    "## 12. Train the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a31721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the BERT model\n",
    "print(\"Training the model...\")\n",
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    epochs=3,  # Reduced epochs due to large dataset size\n",
    "    batch_size=16,\n",
    "    validation_data=(val_inputs, val_labels),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97901106",
   "metadata": {},
   "source": [
    "## 13. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire trained model\n",
    "model.save_pretrained(\"models/bert_multilingual/\")\n",
    "print(\"Model saved to models/bert_multilingual/\")\n",
    "\n",
    "# Also save the tokenizer\n",
    "tokenizer.save_pretrained(\"models/bert_multilingual/\")\n",
    "print(\"Tokenizer saved to models/bert_multilingual/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fecc48",
   "metadata": {},
   "source": [
    "## 14. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df445294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(val_inputs, val_labels, verbose=0)\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da967e",
   "metadata": {},
   "source": [
    "## 15. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c909ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_labels, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de3bbf",
   "metadata": {},
   "source": [
    "## 16. Plot Training and Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ed454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.title('Training and Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/training_history.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b3020",
   "metadata": {},
   "source": [
    "## 17. Generate Predictions and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for the test set\n",
    "predictions = model.predict(test_inputs)\n",
    "predicted_labels = np.argmax(predictions.logits, axis=1)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "# Display confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['HUMAN', 'AI'])\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title('Confusion Matrix - BERT Text Classification', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c611a06",
   "metadata": {},
   "source": [
    "## 18. Calculate ROC Curve and AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aea5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities for positive class (AI)\n",
    "probabilities = tf.nn.softmax(predictions.logits)[:, 1]\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_score = roc_auc_score(test_labels, probabilities)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, probabilities)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.4f}', linewidth=2, color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - BERT Text Classification', fontsize=12, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/roc_curve.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f22a2a",
   "metadata": {},
   "source": [
    "## 19. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification report\n",
    "report = classification_report(\n",
    "    test_labels, \n",
    "    predicted_labels, \n",
    "    target_names=['HUMAN', 'AI'],\n",
    "    digits=4\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT - TEST SET\")\n",
    "print(\"=\"*60)\n",
    "print(report)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c27ee",
   "metadata": {},
   "source": [
    "## 20. Summary of Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a477257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of model performance\n",
    "summary_data = {\n",
    "    'Metric': ['Test Accuracy', 'Test Loss', 'AUC Score'],\n",
    "    'Value': [f'{test_accuracy:.4f}', f'{test_loss:.4f}', f'{auc_score:.4f}']\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('results/model_performance_summary.csv', index=False)\n",
    "print(\"\\nPerformance summary saved to results/model_performance_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
