{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/movindugunarathna/sinXdetect/blob/main/ml/bert_text_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79dbc81f",
      "metadata": {
        "id": "79dbc81f"
      },
      "source": [
        "## 1. Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "827d733c",
      "metadata": {
        "id": "827d733c"
      },
      "outputs": [],
      "source": [
        "%pip install -q tf-keras\n",
        "%pip install -q transformers\n",
        "%pip install -q datasets\n",
        "%pip install -q nltk\n",
        "%pip install -q scikit-learn\n",
        "%pip install -q matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "377243ac",
      "metadata": {
        "id": "377243ac"
      },
      "source": [
        "## 2. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab3d311",
      "metadata": {
        "id": "0ab3d311"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Suppress TensorFlow deprecation warnings\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "373c94de",
      "metadata": {
        "id": "373c94de"
      },
      "source": [
        "## 3. Load Dataset from JSONL Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f345f010",
      "metadata": {
        "id": "f345f010"
      },
      "outputs": [],
      "source": [
        "def get_gdrive_dataset_dir(subpath='dataset'):\n",
        "    \"\"\"Try to mount Google Drive (Colab) and return full path to `subpath`.\n",
        "    Fallbacks:\n",
        "      - Use GDRIVE_DATASET_PATH env var if set and exists.\n",
        "      - Use a local folder named `subpath` in the current working directory.\n",
        "    Raise a clear error if none of the above work.\n",
        "    \"\"\"\n",
        "    # Try Colab mount\n",
        "    try:\n",
        "        from google.colab import drive  # type: ignore\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        base = '/content/drive/MyDrive'\n",
        "        candidate = os.path.join(base, subpath)\n",
        "        if os.path.exists(candidate):\n",
        "            return candidate\n",
        "        # If subpath not found under MyDrive, allow using absolute path provided by user in MyDrive\n",
        "        raise FileNotFoundError(f\"Expected folder at {candidate} not found in Google Drive.\")\n",
        "    except Exception:\n",
        "        # Fallback to environment variable\n",
        "        env_path = os.environ.get('GDRIVE_DATASET_PATH')\n",
        "        if env_path and os.path.exists(env_path):\n",
        "            return env_path\n",
        "        # Fallback to local 'dataset' folder inside current working dir\n",
        "        local_candidate = os.path.join(os.getcwd(), subpath)\n",
        "        if os.path.exists(local_candidate):\n",
        "            return local_candidate\n",
        "        raise RuntimeError(\n",
        "            \"Could not mount Google Drive or locate dataset folder. \"\n",
        "            \"If running locally, set the environment variable GDRIVE_DATASET_PATH \"\n",
        "            \"to the dataset folder path or place a 'dataset' folder in the notebook cwd.\"\n",
        "        )\n",
        "\n",
        "def load_jsonl(filepath):\n",
        "    \"\"\"Load data from JSONL file\"\"\"\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    return data\n",
        "\n",
        "# Resolve dataset directory (will attempt to mount Drive on Colab)\n",
        "dataset_dir = get_gdrive_dataset_dir('dataset')\n",
        "\n",
        "# Load training, validation, and test datasets from the resolved directory\n",
        "train_data = load_jsonl(os.path.join(dataset_dir, 'train.jsonl'))\n",
        "val_data = load_jsonl(os.path.join(dataset_dir, 'val.jsonl'))\n",
        "test_data = load_jsonl(os.path.join(dataset_dir, 'test.jsonl'))\n",
        "\n",
        "print(f\"Dataset directory: {dataset_dir}\")\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Validation set size: {len(val_data)}\")\n",
        "print(f\"Testing set size: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63513a8e",
      "metadata": {
        "id": "63513a8e"
      },
      "source": [
        "## 4. Convert JSONL to DataFrame and Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f880e0",
      "metadata": {
        "id": "a2f880e0"
      },
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "train_df = pd.DataFrame(train_data)\n",
        "val_df = pd.DataFrame(val_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "# Display sample data\n",
        "print(\"Sample training data:\")\n",
        "print(train_df[['text', 'label']].head())\n",
        "print(f\"\\nLabel value counts (Train):\")\n",
        "print(train_df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62d1d0e8",
      "metadata": {
        "id": "62d1d0e8"
      },
      "source": [
        "## 5. Map Labels to Numeric Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0634946",
      "metadata": {
        "id": "d0634946"
      },
      "outputs": [],
      "source": [
        "# Create label mapping\n",
        "label_mapping = {'HUMAN': 0, 'AI': 1}\n",
        "reverse_mapping = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "# Map labels to numeric values\n",
        "train_df['label_encoded'] = train_df['label'].map(label_mapping)\n",
        "val_df['label_encoded'] = val_df['label'].map(label_mapping)\n",
        "test_df['label_encoded'] = test_df['label'].map(label_mapping)\n",
        "\n",
        "# Check for any unmapped values\n",
        "print(f\"Train - Unmapped labels: {train_df['label_encoded'].isna().sum()}\")\n",
        "print(f\"Val - Unmapped labels: {val_df['label_encoded'].isna().sum()}\")\n",
        "print(f\"Test - Unmapped labels: {test_df['label_encoded'].isna().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c185608",
      "metadata": {
        "id": "5c185608"
      },
      "source": [
        "## 6. Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e02bf614",
      "metadata": {
        "id": "e02bf614"
      },
      "outputs": [],
      "source": [
        "def expand_contractions(text):\n",
        "    \"\"\"Expand common English contractions\"\"\"\n",
        "    contractions = {\n",
        "        \"I'm\": \"I am\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"gonna\": \"going to\"\n",
        "    }\n",
        "\n",
        "    for contraction, expanded in contractions.items():\n",
        "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expanded, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply text preprocessing\n",
        "print(\"Preprocessing training data...\")\n",
        "train_df['expanded_text'] = train_df['text'].apply(expand_contractions)\n",
        "\n",
        "print(\"Preprocessing validation data...\")\n",
        "val_df['expanded_text'] = val_df['text'].apply(expand_contractions)\n",
        "\n",
        "print(\"Preprocessing test data...\")\n",
        "test_df['expanded_text'] = test_df['text'].apply(expand_contractions)\n",
        "\n",
        "print(\"Preprocessing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "000ce4ef",
      "metadata": {
        "id": "000ce4ef"
      },
      "source": [
        "## 7. Load BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f100265c",
      "metadata": {
        "id": "f100265c"
      },
      "outputs": [],
      "source": [
        "# Load MobileBERT tokenizer (very lightweight model, ~150MB, supports multilingual)\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/mobilebert-uncased')\n",
        "print(\"MobileBERT tokenizer loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d474d1d",
      "metadata": {
        "id": "4d474d1d"
      },
      "source": [
        "## 8. Tokenize and Encode Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52cb99b6",
      "metadata": {
        "id": "52cb99b6"
      },
      "outputs": [],
      "source": [
        "# Tokenize and encode the text data using tokenizer in batches to avoid memory spikes\n",
        "print(\"Tokenizing data in batches...\")\n",
        "\n",
        "def tokenize_in_batches(texts, tokenizer, batch_size=32, max_length=512):\n",
        "    \"\"\"Tokenize a list of texts in small batches and return numpy arrays.\n",
        "    Returns a dict with 'input_ids' and 'attention_mask' as numpy arrays.\n",
        "    Pads every batch to `max_length` so concatenation shapes match.\n",
        "    \"\"\"\n",
        "    input_ids_parts = []\n",
        "    attention_mask_parts = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        enc = tokenizer(\n",
        "            batch,\n",
        "            padding='max_length',   # pad each batch to the fixed max_length\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        # Convert to numpy to keep memory usage predictable and avoid nested lists\n",
        "        ids = enc['input_ids'].numpy()\n",
        "        mask = enc['attention_mask'].numpy()\n",
        "\n",
        "        # Sanity check: ensure shape[1] == max_length\n",
        "        if ids.shape[1] != max_length:\n",
        "            # If tokenizer produced a different length for some reason, force-pad/truncate\n",
        "            ids = np.pad(ids, ((0,0),(0,max_length-ids.shape[1])), constant_values=0)[:,:max_length]\n",
        "            mask = np.pad(mask, ((0,0),(0,max_length-mask.shape[1])), constant_values=0)[:,:max_length]\n",
        "\n",
        "        input_ids_parts.append(ids)\n",
        "        attention_mask_parts.append(mask)\n",
        "\n",
        "    input_ids = np.concatenate(input_ids_parts, axis=0)\n",
        "    attention_mask = np.concatenate(attention_mask_parts, axis=0)\n",
        "\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "\n",
        "# Run batched tokenization\n",
        "train_encodings = tokenize_in_batches(train_df['expanded_text'].tolist(), tokenizer, batch_size=32)\n",
        "val_encodings = tokenize_in_batches(val_df['expanded_text'].tolist(), tokenizer, batch_size=32)\n",
        "test_encodings = tokenize_in_batches(test_df['expanded_text'].tolist(), tokenizer, batch_size=32)\n",
        "\n",
        "print(\"Tokenization complete!\")\n",
        "print(f\"Training encodings shape: {train_encodings['input_ids'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec9c7630",
      "metadata": {
        "id": "ec9c7630"
      },
      "source": [
        "## 9. Prepare Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e66b2bc6",
      "metadata": {
        "id": "e66b2bc6"
      },
      "outputs": [],
      "source": [
        "# Extract input arrays and convert labels to numpy arrays\n",
        "# Use dicts with both input_ids and attention_mask so the model gets both inputs\n",
        "train_inputs = {\n",
        "    'input_ids': train_encodings['input_ids'],\n",
        "    'attention_mask': train_encodings['attention_mask']\n",
        "}\n",
        "val_inputs = {\n",
        "    'input_ids': val_encodings['input_ids'],\n",
        "    'attention_mask': val_encodings['attention_mask']\n",
        "}\n",
        "test_inputs = {\n",
        "    'input_ids': test_encodings['input_ids'],\n",
        "    'attention_mask': test_encodings['attention_mask']\n",
        "}\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "train_labels = np.array(train_df['label_encoded'].astype(int).tolist())\n",
        "val_labels = np.array(val_df['label_encoded'].astype(int).tolist())\n",
        "test_labels = np.array(test_df['label_encoded'].astype(int).tolist())\n",
        "\n",
        "print(f\"Train labels shape: {train_labels.shape}\")\n",
        "print(f\"Val labels shape: {val_labels.shape}\")\n",
        "print(f\"Test labels shape: {test_labels.shape}\")\n",
        "print(f\"\\nLabel distribution (Train): {np.bincount(train_labels)}\")\n",
        "print(f\"Label distribution (Val): {np.bincount(val_labels)}\")\n",
        "print(f\"Label distribution (Test): {np.bincount(test_labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4522d808",
      "metadata": {
        "id": "4522d808"
      },
      "source": [
        "## 10. Define BERT Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a167267a",
      "metadata": {
        "id": "a167267a"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained MobileBERT model for sequence classification (very lightweight, ~150MB)\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "    'google/mobilebert-uncased',\n",
        "    num_labels=2  # Binary classification: HUMAN (0) vs AI (1)\n",
        ")\n",
        "\n",
        "print(\"MobileBERT model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d250e44",
      "metadata": {
        "id": "1d250e44"
      },
      "source": [
        "## 11. Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e9b0ed7",
      "metadata": {
        "id": "6e9b0ed7"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=[metric]\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully!\")\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8112c1b3",
      "metadata": {
        "id": "8112c1b3"
      },
      "source": [
        "## 12. Train the BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a31721",
      "metadata": {
        "id": "35a31721"
      },
      "outputs": [],
      "source": [
        "# Train the BERT model\n",
        "print(\"Training the model...\")\n",
        "history = model.fit(\n",
        "    train_inputs,\n",
        "    train_labels,\n",
        "    epochs=3,  # Reduced epochs due to large dataset size\n",
        "    batch_size=16,\n",
        "    validation_data=(val_inputs, val_labels),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97901106",
      "metadata": {
        "id": "97901106"
      },
      "source": [
        "## 13. Save the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05db1261",
      "metadata": {
        "id": "05db1261"
      },
      "outputs": [],
      "source": [
        "# Save the entire trained model\n",
        "model.save_pretrained(\"models/bert_multilingual/\")\n",
        "print(\"Model saved to models/bert_multilingual/\")\n",
        "\n",
        "# Also save the tokenizer\n",
        "tokenizer.save_pretrained(\"models/bert_multilingual/\")\n",
        "print(\"Tokenizer saved to models/bert_multilingual/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0fecc48",
      "metadata": {
        "id": "c0fecc48"
      },
      "source": [
        "## 14. Evaluate on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df445294",
      "metadata": {
        "id": "df445294"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = model.evaluate(val_inputs, val_labels, verbose=0)\n",
        "print(f'Validation Loss: {val_loss:.4f}')\n",
        "print(f'Validation Accuracy: {val_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4da967e",
      "metadata": {
        "id": "a4da967e"
      },
      "source": [
        "## 15. Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47c909ba",
      "metadata": {
        "id": "47c909ba"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_inputs, test_labels, verbose=0)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33de3bbf",
      "metadata": {
        "id": "33de3bbf"
      },
      "source": [
        "## 16. Plot Training and Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154ed454",
      "metadata": {
        "id": "154ed454"
      },
      "outputs": [],
      "source": [
        "# Plot accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "plt.title('Training and Validation Accuracy', fontsize=12, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "plt.title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/training_history.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Training history plot saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "427b3020",
      "metadata": {
        "id": "427b3020"
      },
      "source": [
        "## 17. Generate Predictions and Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed0d7df6",
      "metadata": {
        "id": "ed0d7df6"
      },
      "outputs": [],
      "source": [
        "# Predict labels for the test set\n",
        "predictions = model.predict(test_inputs)\n",
        "predicted_labels = np.argmax(predictions.logits, axis=1)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "# Display confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['HUMAN', 'AI'])\n",
        "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "plt.title('Confusion Matrix - BERT Text Classification', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Confusion matrix saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c611a06",
      "metadata": {
        "id": "7c611a06"
      },
      "source": [
        "## 18. Calculate ROC Curve and AUC Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64aea5a3",
      "metadata": {
        "id": "64aea5a3"
      },
      "outputs": [],
      "source": [
        "# Get probabilities for positive class (AI)\n",
        "probabilities = tf.nn.softmax(predictions.logits)[:, 1]\n",
        "\n",
        "# Calculate AUC score\n",
        "auc_score = roc_auc_score(test_labels, probabilities)\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(test_labels, probabilities)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.4f}', linewidth=2, color='blue')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - BERT Text Classification', fontsize=12, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/roc_curve.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"AUC Score: {auc_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33f22a2a",
      "metadata": {
        "id": "33f22a2a"
      },
      "source": [
        "## 19. Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be61bf06",
      "metadata": {
        "id": "be61bf06"
      },
      "outputs": [],
      "source": [
        "# Generate detailed classification report\n",
        "report = classification_report(\n",
        "    test_labels,\n",
        "    predicted_labels,\n",
        "    target_names=['HUMAN', 'AI'],\n",
        "    digits=4\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT - TEST SET\")\n",
        "print(\"=\"*60)\n",
        "print(report)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e8c27ee",
      "metadata": {
        "id": "7e8c27ee"
      },
      "source": [
        "## 20. Summary of Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a477257",
      "metadata": {
        "id": "7a477257"
      },
      "outputs": [],
      "source": [
        "# Create a summary of model performance\n",
        "summary_data = {\n",
        "    'Metric': ['Test Accuracy', 'Test Loss', 'AUC Score'],\n",
        "    'Value': [f'{test_accuracy:.4f}', f'{test_loss:.4f}', f'{auc_score:.4f}']\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Save summary\n",
        "summary_df.to_csv('results/model_performance_summary.csv', index=False)\n",
        "print(\"\\nPerformance summary saved to results/model_performance_summary.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}