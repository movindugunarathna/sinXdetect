{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "40d7715f",
      "metadata": {
        "id": "40d7715f"
      },
      "source": [
        "## Setup\n",
        "- Requires the dataset JSONL files already present in `ml/dataset/` (train/val/test).\n",
        "- Draws on the preprocessing steps from `ml/bert_text_classifier.ipynb` and `My_research_new.ipynb`.\n",
        "- Uses TensorFlow for the BiLSTM and scikit-learn for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605e5a24",
      "metadata": {
        "id": "605e5a24"
      },
      "outputs": [],
      "source": [
        "# Optional: install dependencies if running in a fresh environment\n",
        "# !pip install -q pandas scikit-learn tensorflow matplotlib seaborn joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c1d3bad",
      "metadata": {
        "id": "2c1d3bad",
        "outputId": "4f75b701-60ba-40d7-d6a5-595df25ee07b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68804d68",
      "metadata": {
        "id": "68804d68",
        "outputId": "851f569c-e38e-49d7-d9e3-8ab14ea7e4e3"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Missing train.jsonl",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1083232904.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mTRAIN_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Missing train.jsonl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mVAL_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Missing val.jsonl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mTEST_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Missing test.jsonl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Missing train.jsonl"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths and basic settings\n",
        "DATA_DIR = Path('/dataset')\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Update this path to where your dataset is stored in Google Drive\n",
        "DRIVE_DATA_DIR = Path('/content/drive/MyDrive/dataset')\n",
        "\n",
        "if DRIVE_DATA_DIR.exists():\n",
        "    print(f\"Copying files from {DRIVE_DATA_DIR} to {DATA_DIR}...\")\n",
        "    for filename in ['train.jsonl', 'val.jsonl', 'test.jsonl']:\n",
        "        src_file = DRIVE_DATA_DIR / filename\n",
        "        dst_file = DATA_DIR / filename\n",
        "        if src_file.exists():\n",
        "             shutil.copy(src_file, dst_file)\n",
        "             print(f\"Copied {filename}\")\n",
        "        else:\n",
        "             print(f\"Warning: {filename} not found in Drive source.\")\n",
        "else:\n",
        "    print(f\"Drive path {DRIVE_DATA_DIR} not found. Please adjust the path.\")\n",
        "\n",
        "TRAIN_PATH = DATA_DIR / 'train.jsonl'\n",
        "VAL_PATH = DATA_DIR / 'val.jsonl'\n",
        "TEST_PATH = DATA_DIR / 'test.jsonl'\n",
        "MODEL_DIR = Path('ml/models/bilstm_sinhala')\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "MAX_TOKENS = 30000  # vocab size for TextVectorization\n",
        "SEQ_LEN = 400       # truncate/pad length (tune as needed)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 6\n",
        "EMBED_DIM = 128\n",
        "LSTM_UNITS = 128\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "assert TRAIN_PATH.exists(), 'Missing train.jsonl'\n",
        "assert VAL_PATH.exists(), 'Missing val.jsonl'\n",
        "assert TEST_PATH.exists(), 'Missing test.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cc2d204",
      "metadata": {
        "id": "7cc2d204"
      },
      "outputs": [],
      "source": [
        "# Load JSONL files into DataFrames\n",
        "def read_jsonl(path: Path) -> pd.DataFrame:\n",
        "    return pd.read_json(path, lines=True)\n",
        "\n",
        "train_df = read_jsonl(TRAIN_PATH)\n",
        "val_df = read_jsonl(VAL_PATH)\n",
        "test_df = read_jsonl(TEST_PATH)\n",
        "\n",
        "for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    print(f'{name}: {len(df):,} rows | columns: {list(df.columns)}')\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc5738e",
      "metadata": {
        "id": "2fc5738e"
      },
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_df['label'])\n",
        "\n",
        "def encode_labels(df: pd.DataFrame) -> np.ndarray:\n",
        "    return label_encoder.transform(df['label'])\n",
        "\n",
        "y_train = encode_labels(train_df)\n",
        "y_val = encode_labels(val_df)\n",
        "y_test = encode_labels(test_df)\n",
        "\n",
        "NUM_CLASSES = len(label_encoder.classes_)\n",
        "print('Classes:', label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f311b18",
      "metadata": {
        "id": "1f311b18"
      },
      "outputs": [],
      "source": [
        "# Build TextVectorization for Sinhala text\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=SEQ_LEN,\n",
        "    standardize='lower_and_strip_punctuation'\n",
        ")\n",
        "\n",
        "# Adapt on training text only\n",
        "text_vectorizer.adapt(train_df['text'].values)\n",
        "\n",
        "def make_dataset(texts: pd.Series, labels: np.ndarray, training: bool) -> tf.data.Dataset:\n",
        "    ds = tf.data.Dataset.from_tensor_slices((texts.values, labels))\n",
        "    if training:\n",
        "        ds = ds.shuffle(10000, seed=SEED)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds.map(lambda x, y: (text_vectorizer(x), y))\n",
        "\n",
        "train_ds = make_dataset(train_df['text'], y_train, training=True)\n",
        "val_ds = make_dataset(val_df['text'], y_val, training=False)\n",
        "test_ds = make_dataset(test_df['text'], y_test, training=False)\n",
        "\n",
        "for batch_x, batch_y in train_ds.take(1):\n",
        "    print('Vectorized batch shape:', batch_x.shape, '| labels shape:', batch_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e396f0",
      "metadata": {
        "id": "76e396f0"
      },
      "source": [
        "## Model: Embedding + BiLSTM\n",
        "The network uses an embedding layer initialized randomly, followed by a bidirectional LSTM stack and dropout regularization. The output layer is a dense softmax over the label set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edbd8e0",
      "metadata": {
        "id": "1edbd8e0"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(None,), dtype=tf.int64, name='tokens')\n",
        "    x = tf.keras.layers.Embedding(MAX_TOKENS, EMBED_DIM, mask_zero=True)(inputs)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_UNITS // 2))(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs, outputs, name='bilstm_classifier')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b0d7038",
      "metadata": {
        "id": "8b0d7038"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(MODEL_DIR / 'checkpoint.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max'\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=2,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45682547",
      "metadata": {
        "id": "45682547"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs validation accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a59701",
      "metadata": {
        "id": "49a59701"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the held-out test set\n",
        "test_probs = model.predict(test_ds)\n",
        "test_pred = np.argmax(test_probs, axis=1)\n",
        "\n",
        "print('Test accuracy:', (test_pred == y_test).mean())\n",
        "print('\n",
        "Classification report')\n",
        "print(classification_report(y_test, test_pred, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34379df3",
      "metadata": {
        "id": "34379df3"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44625b76",
      "metadata": {
        "id": "44625b76"
      },
      "outputs": [],
      "source": [
        "# Save model and preprocessing assets\n",
        "model.save(MODEL_DIR / 'saved_model')\n",
        "joblib.dump(label_encoder, MODEL_DIR / 'label_encoder.joblib')\n",
        "# Save the vectorizer config to recreate later\n",
        "vectorizer_config = text_vectorizer.get_config()\n",
        "vectorizer_weights = text_vectorizer.get_weights()\n",
        "with open(MODEL_DIR / 'vectorizer_config.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(vectorizer_config, f)\n",
        "np.savez_compressed(MODEL_DIR / 'vectorizer_weights.npz', *vectorizer_weights)\n",
        "print('Saved to', MODEL_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b32c3c",
      "metadata": {
        "id": "70b32c3c"
      },
      "outputs": [],
      "source": [
        "# Inference helper\n",
        "def predict_texts(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    ds = tf.data.Dataset.from_tensor_slices(texts).batch(BATCH_SIZE)\n",
        "    ds = ds.map(text_vectorizer).prefetch(tf.data.AUTOTUNE)\n",
        "    probs = model.predict(ds)\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "    labels = label_encoder.inverse_transform(preds)\n",
        "    return list(labels)\n",
        "\n",
        "sample_texts = [\n",
        "\n",
        "\n",
        "180\n",
        "371\n",
        "13\n",
        "\n",
        ",\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "print(predict_texts(sample_texts))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}