{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605e5a24",
      "metadata": {
        "id": "605e5a24"
      },
      "outputs": [],
      "source": [
        "# Optional: install dependencies if running in a fresh environment\n",
        "!pip install -q pandas scikit-learn tensorflow matplotlib seaborn joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5af7d0c",
      "metadata": {},
      "source": [
        "## Imports and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c1d3bad",
      "metadata": {
        "id": "2c1d3bad",
        "outputId": "4f75b701-60ba-40d7-d6a5-595df25ee07b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d99205e",
      "metadata": {},
      "source": [
        "## Configuration and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cee87aa",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (if running in Google Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_MOUNTED = True\n",
        "    print(\"Google Drive mounted successfully!\")\n",
        "except ImportError:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Not running in Google Colab. Using local paths.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68804d68",
      "metadata": {
        "id": "68804d68",
        "outputId": "851f569c-e38e-49d7-d9e3-8ab14ea7e4e3"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Missing train.jsonl",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1083232904.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mTRAIN_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Missing train.jsonl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mVAL_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Missing val.jsonl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mTEST_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Missing test.jsonl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Missing train.jsonl"
          ]
        }
      ],
      "source": [
        "# Paths and basic settings\n",
        "# Set base directory based on environment\n",
        "if DRIVE_MOUNTED:\n",
        "    # Google Drive path - Update this to match your Google Drive folder structure\n",
        "    BASE_DIR = Path('/content/drive/MyDrive')\n",
        "    DATA_DIR = BASE_DIR / 'dataset'\n",
        "    MODEL_DIR = BASE_DIR / 'ml/models/bilstm_sinhala'\n",
        "else:\n",
        "    # Local paths\n",
        "    DATA_DIR = Path('ml/dataset')\n",
        "    MODEL_DIR = Path('ml/models/bilstm_sinhala')\n",
        "\n",
        "assert DATA_DIR.exists(), f\"Dataset directory not found at {DATA_DIR.absolute()}\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAIN_PATH = DATA_DIR / 'train.jsonl'\n",
        "VAL_PATH = DATA_DIR / 'val.jsonl'\n",
        "TEST_PATH = DATA_DIR / 'test.jsonl'\n",
        "\n",
        "SEED = 42\n",
        "MAX_TOKENS = 30000  # vocab size for TextVectorization\n",
        "SEQ_LEN = 400       # truncate/pad length (tune as needed)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 6\n",
        "EMBED_DIM = 128\n",
        "LSTM_UNITS = 128\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "assert TRAIN_PATH.exists(), f'Missing train.jsonl at {TRAIN_PATH.absolute()}'\n",
        "assert VAL_PATH.exists(), f'Missing val.jsonl at {VAL_PATH.absolute()}'\n",
        "assert TEST_PATH.exists(), f'Missing test.jsonl at {TEST_PATH.absolute()}'\n",
        "\n",
        "print(f\"Data directory: {DATA_DIR.absolute()}\")\n",
        "print(f\"Model directory: {MODEL_DIR.absolute()}\")\n",
        "print(f\"Train path: {TRAIN_PATH.absolute()}\")\n",
        "print(f\"Val path: {VAL_PATH.absolute()}\")\n",
        "print(f\"Test path: {TEST_PATH.absolute()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58b1533c",
      "metadata": {},
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cc2d204",
      "metadata": {
        "id": "7cc2d204"
      },
      "outputs": [],
      "source": [
        "# Load JSONL files into DataFrames\n",
        "def read_jsonl(path: Path) -> pd.DataFrame:\n",
        "    return pd.read_json(path, lines=True)\n",
        "\n",
        "train_df = read_jsonl(TRAIN_PATH)\n",
        "val_df = read_jsonl(VAL_PATH)\n",
        "test_df = read_jsonl(TEST_PATH)\n",
        "\n",
        "for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    print(f'{name}: {len(df):,} rows | columns: {list(df.columns)}')\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5140054e",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc5738e",
      "metadata": {
        "id": "2fc5738e"
      },
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_df['label'])\n",
        "\n",
        "def encode_labels(df: pd.DataFrame) -> np.ndarray:\n",
        "    return label_encoder.transform(df['label'])\n",
        "\n",
        "y_train = encode_labels(train_df)\n",
        "y_val = encode_labels(val_df)\n",
        "y_test = encode_labels(test_df)\n",
        "\n",
        "NUM_CLASSES = len(label_encoder.classes_)\n",
        "print('Classes:', label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f311b18",
      "metadata": {
        "id": "1f311b18"
      },
      "outputs": [],
      "source": [
        "# Build TextVectorization for Sinhala text\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=SEQ_LEN,\n",
        "    standardize='lower_and_strip_punctuation'\n",
        ")\n",
        "\n",
        "# Adapt on training text only\n",
        "text_vectorizer.adapt(train_df['text'].values)\n",
        "\n",
        "def make_dataset(texts: pd.Series, labels: np.ndarray, training: bool) -> tf.data.Dataset:\n",
        "    ds = tf.data.Dataset.from_tensor_slices((texts.values, labels))\n",
        "    if training:\n",
        "        ds = ds.shuffle(10000, seed=SEED)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds.map(lambda x, y: (text_vectorizer(x), y))\n",
        "\n",
        "train_ds = make_dataset(train_df['text'], y_train, training=True)\n",
        "val_ds = make_dataset(val_df['text'], y_val, training=False)\n",
        "test_ds = make_dataset(test_df['text'], y_test, training=False)\n",
        "\n",
        "for batch_x, batch_y in train_ds.take(1):\n",
        "    print('Vectorized batch shape:', batch_x.shape, '| labels shape:', batch_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e396f0",
      "metadata": {
        "id": "76e396f0"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "The network uses an embedding layer initialized randomly, followed by a bidirectional LSTM stack and dropout regularization. The output layer is a dense softmax over the label set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edbd8e0",
      "metadata": {
        "id": "1edbd8e0"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(None,), dtype=tf.int64, name='tokens')\n",
        "    x = tf.keras.layers.Embedding(MAX_TOKENS, EMBED_DIM, mask_zero=True)(inputs)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_UNITS // 2))(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs, outputs, name='bilstm_classifier')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c446553",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b0d7038",
      "metadata": {
        "id": "8b0d7038"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(MODEL_DIR / 'checkpoint.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max'\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=2,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45682547",
      "metadata": {
        "id": "45682547"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs validation accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa73c5c",
      "metadata": {},
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a59701",
      "metadata": {
        "id": "49a59701"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the held-out test set\n",
        "test_probs = model.predict(test_ds)\n",
        "test_pred = np.argmax(test_probs, axis=1)\n",
        "\n",
        "print('Test accuracy:', (test_pred == y_test).mean())\n",
        "print('Classification report')\n",
        "print(classification_report(y_test, test_pred, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34379df3",
      "metadata": {
        "id": "34379df3"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1081d1da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curve\n",
        "# For multi-class ROC curve (one-vs-rest)\n",
        "y_test_bin = label_binarize(y_test, classes=range(NUM_CLASSES))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "if NUM_CLASSES == 2:\n",
        "    # Binary classification\n",
        "    fpr, tpr, _ = roc_curve(y_test, test_probs[:, 1])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "else:\n",
        "    # Multi-class: plot ROC for each class\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, NUM_CLASSES))\n",
        "    for i in range(NUM_CLASSES):\n",
        "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], test_probs[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
        "                label=f'{label_encoder.classes_[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "916b0b71",
      "metadata": {},
      "source": [
        "## Model Persistence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44625b76",
      "metadata": {
        "id": "44625b76"
      },
      "outputs": [],
      "source": [
        "# Save model and preprocessing assets\n",
        "model.save(MODEL_DIR / 'saved_model')\n",
        "joblib.dump(label_encoder, MODEL_DIR / 'label_encoder.joblib')\n",
        "# Save the vectorizer config to recreate later\n",
        "vectorizer_config = text_vectorizer.get_config()\n",
        "vectorizer_weights = text_vectorizer.get_weights()\n",
        "with open(MODEL_DIR / 'vectorizer_config.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(vectorizer_config, f)\n",
        "np.savez_compressed(MODEL_DIR / 'vectorizer_weights.npz', *vectorizer_weights)\n",
        "print('Saved to', MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95dcc724",
      "metadata": {},
      "source": [
        "## Inference and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b32c3c",
      "metadata": {
        "id": "70b32c3c"
      },
      "outputs": [],
      "source": [
        "# Inference helper\n",
        "def predict_texts(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    ds = tf.data.Dataset.from_tensor_slices(texts).batch(BATCH_SIZE)\n",
        "    ds = ds.map(text_vectorizer).prefetch(tf.data.AUTOTUNE)\n",
        "    probs = model.predict(ds)\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "    labels = label_encoder.inverse_transform(preds)\n",
        "    confidences = np.max(probs, axis=1)\n",
        "    return list(zip(labels, confidences))\n",
        "\n",
        "# Test with sample texts from the test set\n",
        "sample_indices = np.random.choice(len(test_df), 3, replace=False)\n",
        "sample_texts = test_df.iloc[sample_indices]['text'].tolist()\n",
        "predictions = predict_texts(sample_texts)\n",
        "\n",
        "for i, (text, (pred_label, confidence)) in enumerate(zip(sample_texts, predictions)):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Text: {text[:100]}...\")\n",
        "    print(f\"Predicted: {pred_label} (confidence: {confidence:.4f})\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "julia 1.11",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "julia",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
