{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7a141afd",
      "metadata": {},
      "source": [
        "# BiLSTM Text Classifier with Leakage Detection & Proper Evaluation\n",
        "\n",
        "This notebook trains a bidirectional LSTM classifier on Sinhala text with:\n",
        "- **Leakage detection** to catch data contamination\n",
        "- **Proper data split pipeline** (train-only adaptation)\n",
        "- **Generator-holdout evaluation** to simulate production conditions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605e5a24",
      "metadata": {
        "id": "605e5a24"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mKernel julia 1.11 is not usable. Check the Jupyter output tab for more information. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Optional: install dependencies if running in a fresh environment\n",
        "!pip install -q pandas scikit-learn tensorflow matplotlib seaborn joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5af7d0c",
      "metadata": {},
      "source": [
        "## Imports and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c1d3bad",
      "metadata": {
        "id": "2c1d3bad",
        "outputId": "4f75b701-60ba-40d7-d6a5-595df25ee07b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d99205e",
      "metadata": {},
      "source": [
        "## Configuration and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cee87aa",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (if running in Google Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_MOUNTED = True\n",
        "    print(\"Google Drive mounted successfully!\")\n",
        "except ImportError:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Not running in Google Colab. Using local paths.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68804d68",
      "metadata": {
        "id": "68804d68",
        "outputId": "851f569c-e38e-49d7-d9e3-8ab14ea7e4e3"
      },
      "outputs": [],
      "source": [
        "# Paths and basic settings\n",
        "# Set base directory based on environment\n",
        "if DRIVE_MOUNTED:\n",
        "    # Google Drive path - Update this to match your Google Drive folder structure\n",
        "    BASE_DIR = Path('/content/drive/MyDrive/Colab Notebooks/')\n",
        "    DATA_DIR = BASE_DIR / 'dataset'\n",
        "    MODEL_DIR = BASE_DIR / 'ml/models/bilstm_sinhala'\n",
        "else:\n",
        "    # Local paths\n",
        "    DATA_DIR = Path('ml/dataset')\n",
        "    MODEL_DIR = Path('ml/models/bilstm_sinhala')\n",
        "\n",
        "assert DATA_DIR.exists(), f\"Dataset directory not found at {DATA_DIR.absolute()}\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAIN_PATH = DATA_DIR / 'train.jsonl'\n",
        "VAL_PATH = DATA_DIR / 'val.jsonl'\n",
        "TEST_PATH = DATA_DIR / 'test.jsonl'\n",
        "\n",
        "SEED = 42\n",
        "MAX_TOKENS = 30000  # vocab size for TextVectorization\n",
        "SEQ_LEN = 400       # truncate/pad length (tune as needed)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 6\n",
        "EMBED_DIM = 128\n",
        "LSTM_UNITS = 128\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "assert TRAIN_PATH.exists(), f'Missing train.jsonl at {TRAIN_PATH.absolute()}'\n",
        "assert VAL_PATH.exists(), f'Missing val.jsonl at {VAL_PATH.absolute()}'\n",
        "assert TEST_PATH.exists(), f'Missing test.jsonl at {TEST_PATH.absolute()}'\n",
        "\n",
        "print(f\"Data directory: {DATA_DIR.absolute()}\")\n",
        "print(f\"Model directory: {MODEL_DIR.absolute()}\")\n",
        "print(f\"Train path: {TRAIN_PATH.absolute()}\")\n",
        "print(f\"Val path: {VAL_PATH.absolute()}\")\n",
        "print(f\"Test path: {TEST_PATH.absolute()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58b1533c",
      "metadata": {},
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cc2d204",
      "metadata": {
        "id": "7cc2d204"
      },
      "outputs": [],
      "source": [
        "# Load JSONL files into DataFrames\n",
        "def read_jsonl(path: Path) -> pd.DataFrame:\n",
        "    return pd.read_json(path, lines=True)\n",
        "\n",
        "train_df = read_jsonl(TRAIN_PATH)\n",
        "val_df = read_jsonl(VAL_PATH)\n",
        "test_df = read_jsonl(TEST_PATH)\n",
        "\n",
        "for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    print(f'{name}: {len(df):,} rows | columns: {list(df.columns)}')\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19fd4ea6",
      "metadata": {},
      "source": [
        "## Leakage Detection\n",
        "\n",
        "Check for duplicate samples across train/val/test splits and verify vectorizer fit scope.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "447e61e3",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Leakage detection\n",
        "def detect_leakage(train_df, val_df, test_df):\n",
        "    \"\"\"Detect data leakage across splits.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"LEAKAGE DETECTION REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Check for duplicates within each split\n",
        "    for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "        dup_count = df['text'].duplicated().sum()\n",
        "        print(f\"\\n{name.upper()} SET:\")\n",
        "        print(f\"  Total samples: {len(df)}\")\n",
        "        print(f\"  Duplicates within split: {dup_count}\")\n",
        "        if dup_count > 0:\n",
        "            print(f\"  ⚠️  WARNING: {dup_count} duplicate texts in {name} set!\")\n",
        "    \n",
        "    # Check for overlaps between splits\n",
        "    train_texts = set(train_df['text'].values)\n",
        "    val_texts = set(val_df['text'].values)\n",
        "    test_texts = set(test_df['text'].values)\n",
        "    \n",
        "    train_val_overlap = len(train_texts & val_texts)\n",
        "    train_test_overlap = len(train_texts & test_texts)\n",
        "    val_test_overlap = len(val_texts & test_texts)\n",
        "    \n",
        "    print(f\"\\nCROSS-SPLIT OVERLAPS:\")\n",
        "    print(f\"  Train-Val overlap: {train_val_overlap} samples\")\n",
        "    if train_val_overlap > 0:\n",
        "        print(f\"  ⚠️  CRITICAL LEAKAGE DETECTED in Train-Val!\")\n",
        "    \n",
        "    print(f\"  Train-Test overlap: {train_test_overlap} samples\")\n",
        "    if train_test_overlap > 0:\n",
        "        print(f\"  ⚠️  CRITICAL LEAKAGE DETECTED in Train-Test!\")\n",
        "    \n",
        "    print(f\"  Val-Test overlap: {val_test_overlap} samples\")\n",
        "    if val_test_overlap > 0:\n",
        "        print(f\"  ⚠️  CRITICAL LEAKAGE DETECTED in Val-Test!\")\n",
        "    \n",
        "    total_leakage = train_val_overlap + train_test_overlap + val_test_overlap\n",
        "    if total_leakage == 0:\n",
        "        print(f\"\\n✓ NO LEAKAGE DETECTED\")\n",
        "    else:\n",
        "        print(f\"\\n✗ TOTAL LEAKAGE: {total_leakage} samples across splits\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    return total_leakage == 0\n",
        "\n",
        "# Run leakage detection\n",
        "is_clean = detect_leakage(train_df, val_df, test_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5140054e",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc5738e",
      "metadata": {
        "id": "2fc5738e"
      },
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_df['label'])\n",
        "\n",
        "def encode_labels(df: pd.DataFrame) -> np.ndarray:\n",
        "    return label_encoder.transform(df['label'])\n",
        "\n",
        "y_train = encode_labels(train_df)\n",
        "y_val = encode_labels(val_df)\n",
        "y_test = encode_labels(test_df)\n",
        "\n",
        "NUM_CLASSES = len(label_encoder.classes_)\n",
        "print('Classes:', label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f311b18",
      "metadata": {
        "id": "1f311b18"
      },
      "outputs": [],
      "source": [
        "# Build TextVectorization ONLY on training data (prevents leakage)\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=SEQ_LEN,\n",
        "    standardize='lower_and_strip_punctuation'\n",
        ")\n",
        "\n",
        "# CRITICAL: Adapt ONLY on training text\n",
        "print(\"Adapting vectorizer on TRAINING data only...\")\n",
        "text_vectorizer.adapt(train_df['text'].values)\n",
        "print(f\"Vectorizer vocabulary size: {text_vectorizer.vocabulary_size()}\")\n",
        "\n",
        "def make_dataset(texts, labels, training=False):\n",
        "    \"\"\"Create tf.data pipeline. Vectorization already fitted on train only.\"\"\"\n",
        "    ds = tf.data.Dataset.from_tensor_slices((texts.values, labels))\n",
        "    if training:\n",
        "        ds = ds.shuffle(10000, seed=SEED)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds.map(lambda x, y: (text_vectorizer(x), y))\n",
        "\n",
        "train_ds = make_dataset(train_df['text'], y_train, training=True)\n",
        "val_ds = make_dataset(val_df['text'], y_val, training=False)\n",
        "test_ds = make_dataset(test_df['text'], y_test, training=False)\n",
        "\n",
        "for batch_x, batch_y in train_ds.take(1):\n",
        "    print('Vectorized batch shape:', batch_x.shape, '| labels shape:', batch_y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e396f0",
      "metadata": {
        "id": "76e396f0"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "The network uses an embedding layer initialized randomly, followed by a bidirectional LSTM stack and dropout regularization. The output layer is a dense softmax over the label set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edbd8e0",
      "metadata": {
        "id": "1edbd8e0"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(None,), dtype=tf.int64, name='tokens')\n",
        "    x = tf.keras.layers.Embedding(MAX_TOKENS, EMBED_DIM, mask_zero=True)(inputs)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_UNITS // 2))(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs, outputs, name='bilstm_classifier')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c446553",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b0d7038",
      "metadata": {
        "id": "8b0d7038"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(MODEL_DIR / 'checkpoint.keras'),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max'\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=2,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45682547",
      "metadata": {
        "id": "45682547"
      },
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs validation accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa73c5c",
      "metadata": {},
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a59701",
      "metadata": {
        "id": "49a59701"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the held-out test set\n",
        "test_probs = model.predict(test_ds)\n",
        "test_pred = np.argmax(test_probs, axis=1)\n",
        "\n",
        "print('Test accuracy:', (test_pred == y_test).mean())\n",
        "print('Classification report')\n",
        "print(classification_report(y_test, test_pred, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34379df3",
      "metadata": {
        "id": "34379df3"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1081d1da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curve\n",
        "# For multi-class ROC curve (one-vs-rest)\n",
        "y_test_bin = label_binarize(y_test, classes=range(NUM_CLASSES))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "if NUM_CLASSES == 2:\n",
        "    # Binary classification\n",
        "    fpr, tpr, _ = roc_curve(y_test, test_probs[:, 1])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "else:\n",
        "    # Multi-class: plot ROC for each class\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, NUM_CLASSES))\n",
        "    for i in range(NUM_CLASSES):\n",
        "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], test_probs[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
        "                label=f'{label_encoder.classes_[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "916b0b71",
      "metadata": {},
      "source": [
        "## Model Persistence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44625b76",
      "metadata": {
        "id": "44625b76"
      },
      "outputs": [],
      "source": [
        "# Save model and preprocessing assets\n",
        "model.save(MODEL_DIR / 'saved_model')\n",
        "joblib.dump(label_encoder, MODEL_DIR / 'label_encoder.joblib')\n",
        "# Save the vectorizer config to recreate later\n",
        "vectorizer_config = text_vectorizer.get_config()\n",
        "vectorizer_weights = text_vectorizer.get_weights()\n",
        "with open(MODEL_DIR / 'vectorizer_config.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(vectorizer_config, f)\n",
        "np.savez_compressed(MODEL_DIR / 'vectorizer_weights.npz', *vectorizer_weights)\n",
        "print('Saved to', MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95dcc724",
      "metadata": {},
      "source": [
        "## Inference and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b32c3c",
      "metadata": {
        "id": "70b32c3c"
      },
      "outputs": [],
      "source": [
        "# Safe inference helper (generator-style)\n",
        "def predict_texts(texts, batch_size=32):\n",
        "    \"\"\"\n",
        "    Predict on texts using streaming generator pattern.\n",
        "    Safe for production and avoids batch-size artifacts.\n",
        "    \"\"\"\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    \n",
        "    results = []\n",
        "    num_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(texts))\n",
        "        batch = texts[start_idx:end_idx]\n",
        "        \n",
        "        ds = tf.data.Dataset.from_tensor_slices(batch).batch(len(batch))\n",
        "        ds = ds.map(text_vectorizer).prefetch(tf.data.AUTOTUNE)\n",
        "        probs = model.predict(ds, verbose=0)\n",
        "        preds = np.argmax(probs, axis=1)\n",
        "        labels = label_encoder.inverse_transform(preds)\n",
        "        confidences = np.max(probs, axis=1)\n",
        "        \n",
        "        results.extend(zip(labels, confidences, probs))\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test with sample texts from the test set\n",
        "sample_indices = np.random.choice(len(test_df), min(3, len(test_df)), replace=False)\n",
        "sample_texts = test_df.iloc[sample_indices]['text'].tolist()\n",
        "predictions = predict_texts(sample_texts)\n",
        "\n",
        "for i, (text, (pred_label, confidence, probs)) in enumerate(zip(sample_texts, predictions)):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Text: {text[:100]}...\")\n",
        "    print(f\"Predicted: {pred_label} (confidence: {confidence:.4f})\")\n",
        "    print(f\"Class probabilities: {dict(zip(label_encoder.classes_, probs))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dc020db",
      "metadata": {},
      "source": [
        "## Generator-Holdout Evaluation\n",
        "\n",
        "Simulate production conditions by evaluating on a streaming generator without batch effects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6127e0",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def evaluate_generator_holdout(model, test_df, y_test, batch_size=32):\n",
        "    \"\"\"\n",
        "    Generator-based holdout evaluation: process test data in small batches\n",
        "    to simulate production streaming conditions and avoid batch-size artifacts.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"GENERATOR-HOLDOUT EVALUATION\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    all_probs = []\n",
        "    all_preds = []\n",
        "    \n",
        "    # Process in smaller generator batches\n",
        "    num_batches = (len(test_df) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(test_df))\n",
        "        \n",
        "        batch_texts = test_df['text'].iloc[start_idx:end_idx].values\n",
        "        batch_labels = y_test[start_idx:end_idx]\n",
        "        \n",
        "        # Create single-batch dataset for this generator batch\n",
        "        batch_ds = tf.data.Dataset.from_tensor_slices(batch_texts).batch(len(batch_texts))\n",
        "        batch_ds = batch_ds.map(text_vectorizer).prefetch(tf.data.AUTOTUNE)\n",
        "        \n",
        "        # Predict on this batch\n",
        "        batch_probs = model.predict(batch_ds, verbose=0)\n",
        "        all_probs.append(batch_probs)\n",
        "        all_preds.extend(np.argmax(batch_probs, axis=1))\n",
        "    \n",
        "    # Concatenate all predictions\n",
        "    test_probs = np.vstack(all_probs)\n",
        "    test_pred = np.array(all_preds)\n",
        "    \n",
        "    # Compute metrics\n",
        "    accuracy = (test_pred == y_test).mean()\n",
        "    \n",
        "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, test_pred, target_names=label_encoder.classes_))\n",
        "    \n",
        "    # Compute AUC per class\n",
        "    y_test_bin = label_binarize(y_test, classes=range(NUM_CLASSES))\n",
        "    if NUM_CLASSES == 2:\n",
        "        fpr, tpr, _ = roc_curve(y_test, test_probs[:, 1])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        print(f\"\\nROC AUC (Binary): {roc_auc:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\nPer-class ROC AUC:\")\n",
        "        for i in range(NUM_CLASSES):\n",
        "            fpr, tpr, _ = roc_curve(y_test_bin[:, i], test_probs[:, i])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            print(f\"  {label_encoder.classes_[i]}: {roc_auc:.4f}\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return test_probs, test_pred\n",
        "\n",
        "# Run generator-holdout evaluation\n",
        "test_probs_gen, test_pred_gen = evaluate_generator_holdout(model, test_df, y_test)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "julia 1.11",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "julia",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
