{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24bbc111",
   "metadata": {},
   "source": [
    "# XLM-RoBERTa Large Text Classifier for Sinhala AI/Human Detection\n",
    "\n",
    "Text classification model using **XLM-RoBERTa Large** for detecting AI-generated vs Human-written Sinhala text. XLM-RoBERTa is specifically designed for multilingual tasks and performs excellently with low-resource languages like Sinhala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb435025",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tf-keras\n",
    "%pip install -q transformers\n",
    "%pip install -q datasets\n",
    "%pip install -q nltk\n",
    "%pip install -q scikit-learn\n",
    "%pip install -q matplotlib\n",
    "%pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5b499",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19943840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress TensorFlow deprecation warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc9cbf",
   "metadata": {},
   "source": [
    "## 3. Load Dataset from JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load data from JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Load training, validation, and test datasets\n",
    "train_data = load_jsonl('dataset/train.jsonl')\n",
    "val_data = load_jsonl('dataset/val.jsonl')\n",
    "test_data = load_jsonl('dataset/test.jsonl')\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Testing set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138036f",
   "metadata": {},
   "source": [
    "## 4. Convert JSONL to DataFrame and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c5f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Display sample data\n",
    "print(\"Sample training data:\")\n",
    "print(train_df[['text', 'label']].head())\n",
    "print(f\"\\nLabel value counts (Train):\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4af16",
   "metadata": {},
   "source": [
    "## 5. Map Labels to Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bec6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mapping\n",
    "label_mapping = {'HUMAN': 0, 'AI': 1}\n",
    "reverse_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Map labels to numeric values\n",
    "train_df['label_encoded'] = train_df['label'].map(label_mapping)\n",
    "val_df['label_encoded'] = val_df['label'].map(label_mapping)\n",
    "test_df['label_encoded'] = test_df['label'].map(label_mapping)\n",
    "\n",
    "# Check for any unmapped values\n",
    "print(f\"Train - Unmapped labels: {train_df['label_encoded'].isna().sum()}\")\n",
    "print(f\"Val - Unmapped labels: {val_df['label_encoded'].isna().sum()}\")\n",
    "print(f\"Test - Unmapped labels: {test_df['label_encoded'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ec5d4e",
   "metadata": {},
   "source": [
    "## 6. Text Preprocessing\n",
    "\n",
    "XLM-RoBERTa handles multilingual text well, but we'll still apply Sinhala-specific normalization for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Sinhala-focused normalization & contraction expansion\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \"\"\"\n",
    "    Normalize Sinhala text and expand a small set of common colloquial contractions.\n",
    "    - NFC Unicode normalization (avoids mixed composition forms)\n",
    "    - Remove Zero-Width Joiner/Non-Joiner which can appear in Sinhala typing\n",
    "    - Expand a curated list of common colloquial forms to their standard forms\n",
    "    Note: This list is intentionally conservative to avoid changing semantics.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Normalize to NFC to standardize diacritics\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "\n",
    "    # Remove zero-width characters often accidentally present\n",
    "    text = text.replace('\\u200c', '').replace('\\u200d', '')  # ZWNJ/ZWJ\n",
    "\n",
    "    # Conservative Sinhala \"contractions\" / colloquial expansions\n",
    "    contractions_si = {\n",
    "        # Negation forms\n",
    "        'නෑ': 'නැහැ',\n",
    "        'බෑ': 'බැහැ',\n",
    "        # Colloquial vowels / emphasis\n",
    "        'දැං': 'දැන්',\n",
    "        'කොහේද': 'කොහෙද',\n",
    "        'මොකෝ': 'මොකද',\n",
    "        # Common short forms ending with \"ෙ\" to \"ේ\" (targeted words only)\n",
    "        'ඇතුලෙ': 'ඇතුලේ',\n",
    "        'බාහිරෙ': 'බාහිරේ',\n",
    "        'වැඩෙ': 'වැඩේ',\n",
    "        'රජයෙ': 'රජයේ',\n",
    "        'යාලුවෙ': 'යාලුවේ',\n",
    "    }\n",
    "\n",
    "    # Apply word-boundary replacements for items above\n",
    "    for contraction, expanded in contractions_si.items():\n",
    "        text = re.sub(r\"\\b\" + re.escape(contraction) + r\"\\b\", expanded, text)\n",
    "\n",
    "    # Targeted possessive expansions (avoid generic \"ගෙ\" → \"ගේ\" to not corrupt 'ගෙදර')\n",
    "    text = re.sub(r\"\\bමගෙ\\b\", \"මගේ\", text)\n",
    "    text = re.sub(r\"\\bඔයාගෙ\\b\", \"ඔයාගේ\", text)\n",
    "    text = re.sub(r\"\\bඔගෙ\\b\", \"ඔගේ\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply expansion of contractions using Sinhala-focused normalization\n",
    "print(\"Preprocessing training data (Sinhala normalization)...\")\n",
    "train_df['expanded_text'] = train_df['text'].apply(expand_contractions)\n",
    "\n",
    "print(\"Preprocessing validation data (Sinhala normalization)...\")\n",
    "val_df['expanded_text'] = val_df['text'].apply(expand_contractions)\n",
    "\n",
    "print(\"Preprocessing test data (Sinhala normalization)...\")\n",
    "test_df['expanded_text'] = test_df['text'].apply(expand_contractions)\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8905a1",
   "metadata": {},
   "source": [
    "## 7. Load XLM-RoBERTa Tokenizer\n",
    "\n",
    "XLM-RoBERTa is trained on 100 languages including Sinhala. We'll use the base model (~560MB) which provides excellent multilingual performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3204ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XLM-RoBERTa Large tokenizer\n",
    "model_name = 'xlm-roberta-large'  # ~2.2GB, 550M parameters, best multilingual performance\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"XLM-RoBERTa Large tokenizer loaded successfully from {model_name}!\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04af91b",
   "metadata": {},
   "source": [
    "## 8. Tokenize and Encode Text Data\n",
    "\n",
    "Using **max_length=256** for better generalization and larger effective batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca122ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the text data using tokenizer in batches to avoid memory spikes\n",
    "print(\"Tokenizing data in batches...\")\n",
    "\n",
    "MAX_LENGTH = 256  # Optimal length for better generalization and GPU efficiency\n",
    "\n",
    "def tokenize_in_batches(texts, tokenizer, batch_size=32, max_length=MAX_LENGTH):\n",
    "    \"\"\"Tokenize a list of texts in small batches and return numpy arrays.\n",
    "    Returns a dict with 'input_ids' and 'attention_mask' as numpy arrays.\n",
    "    Pads every batch to `max_length` so concatenation shapes match.\n",
    "    \"\"\"\n",
    "    input_ids_parts = []\n",
    "    attention_mask_parts = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            padding='max_length',   # pad each batch to the fixed max_length\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        # Convert to numpy to keep memory usage predictable and avoid nested lists\n",
    "        ids = enc['input_ids'].numpy()\n",
    "        mask = enc['attention_mask'].numpy()\n",
    "\n",
    "        # Sanity check: ensure shape[1] == max_length\n",
    "        if ids.shape[1] != max_length:\n",
    "            # If tokenizer produced a different length for some reason, force-pad/truncate\n",
    "            ids = np.pad(ids, ((0,0),(0,max_length-ids.shape[1])), constant_values=0)[:,:max_length]\n",
    "            mask = np.pad(mask, ((0,0),(0,max_length-mask.shape[1])), constant_values=0)[:,:max_length]\n",
    "\n",
    "        input_ids_parts.append(ids)\n",
    "        attention_mask_parts.append(mask)\n",
    "\n",
    "    input_ids = np.concatenate(input_ids_parts, axis=0)\n",
    "    attention_mask = np.concatenate(attention_mask_parts, axis=0)\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "# Run batched tokenization\n",
    "train_encodings = tokenize_in_batches(train_df['expanded_text'].tolist(), tokenizer, batch_size=32)\n",
    "val_encodings = tokenize_in_batches(val_df['expanded_text'].tolist(), tokenizer, batch_size=32)\n",
    "test_encodings = tokenize_in_batches(test_df['expanded_text'].tolist(), tokenizer, batch_size=32)\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "print(f\"Training encodings shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"Validation encodings shape: {val_encodings['input_ids'].shape}\")\n",
    "print(f\"Test encodings shape: {test_encodings['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce058cd1",
   "metadata": {},
   "source": [
    "## 9. Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9941e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract input arrays and convert labels to numpy arrays\n",
    "# Use dicts with both input_ids and attention_mask so the model gets both inputs\n",
    "train_inputs = {\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask']\n",
    "}\n",
    "val_inputs = {\n",
    "    'input_ids': val_encodings['input_ids'],\n",
    "    'attention_mask': val_encodings['attention_mask']\n",
    "}\n",
    "test_inputs = {\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask']\n",
    "}\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "train_labels = np.array(train_df['label_encoded'].astype(int).tolist())\n",
    "val_labels = np.array(val_df['label_encoded'].astype(int).tolist())\n",
    "test_labels = np.array(test_df['label_encoded'].astype(int).tolist())\n",
    "\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Val labels shape: {val_labels.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "print(f\"\\nLabel distribution (Train): {np.bincount(train_labels)}\")\n",
    "print(f\"Label distribution (Val): {np.bincount(val_labels)}\")\n",
    "print(f\"Label distribution (Test): {np.bincount(test_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3d24b",
   "metadata": {},
   "source": [
    "## 10. Define XLM-RoBERTa Classification Model\n",
    "\n",
    "Loading the pre-trained XLM-RoBERTa model for sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028cb432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained XLM-RoBERTa Large model for sequence classification\n",
    "print(\"Loading XLM-RoBERTa Large model (this may take a moment)...\")\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.2,\n",
    "    attention_probs_dropout_prob=0.2\n",
    ")\n",
    "\n",
    "print(f\"XLM-RoBERTa Large model loaded successfully from {model_name}!\")\n",
    "print(f\"Model has {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fec05d",
   "metadata": {},
   "source": [
    "## 11. Compile the Model with Optimized Settings\n",
    "\n",
    "Using:\n",
    "- **Learning rate:** 1.5e-5 (optimal for Large model)\n",
    "- **Weight decay:** 0.01 (L2 regularization)\n",
    "- **Gradient clipping:** 1.0 (prevents exploding gradients)\n",
    "- **Mixed precision:** fp16 (faster training on modern GPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542dc630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable mixed precision training for faster training (requires compatible GPU)\n",
    "try:\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision (fp16) enabled!\")\n",
    "except:\n",
    "    print(\"Mixed precision not available, using default precision\")\n",
    "\n",
    "# Compile the model with optimized hyperparameters for XLM-RoBERTa Large\n",
    "from transformers import create_optimizer\n",
    "\n",
    "num_train_steps = len(train_labels) // 16 * 6\n",
    "num_warmup_steps = int(0.08 * num_train_steps)\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=1.5e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    weight_decay_rate=0.01\n",
    ")\n",
    "\n",
    "optimizer.clipnorm = 1.0\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[metric]\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(f\"Total training steps: {num_train_steps}\")\n",
    "print(f\"Warmup steps: {num_warmup_steps}\")\n",
    "print(f\"Learning rate: 1.5e-5\")\n",
    "print(f\"Weight decay: 0.01\")\n",
    "print(f\"Gradient clipping: 1.0\")\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1273182",
   "metadata": {},
   "source": [
    "## 12. Calculate Class Weights and Train the Model\n",
    "\n",
    "Using:\n",
    "- **Class weights** to improve AI recall (addresses class imbalance)\n",
    "- **Early stopping** with patience=2 to prevent overfitting\n",
    "- **Gradient accumulation** (effective batch size = 32)\n",
    "- **6 epochs** with validation monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a80304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to improve AI recall\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "class_weight = {\n",
    "    0: class_weights_array[0],\n",
    "    1: class_weights_array[1] * 1.2\n",
    "}\n",
    "\n",
    "print(f\"Class weights: {class_weight}\")\n",
    "print(f\"This helps the model focus more on the AI class to improve recall.\\n\")\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training the XLM-RoBERTa Large model...\")\n",
    "print(\"This may take some time depending on your hardware.\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    epochs=6,\n",
    "    batch_size=16,\n",
    "    validation_data=(val_inputs, val_labels),\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best epoch: {len(history.history['loss']) - early_stopping.patience if early_stopping.stopped_epoch > 0 else len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b6193",
   "metadata": {},
   "source": [
    "## 13. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe207fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"models/xlm_roberta_classifier\", exist_ok=True)\n",
    "\n",
    "# Save the entire trained model\n",
    "model.save_pretrained(\"models/xlm_roberta_classifier/\")\n",
    "print(\"Model saved to models/xlm_roberta_classifier/\")\n",
    "\n",
    "# Also save the tokenizer\n",
    "tokenizer.save_pretrained(\"models/xlm_roberta_classifier/\")\n",
    "print(\"Tokenizer saved to models/xlm_roberta_classifier/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2e4d7",
   "metadata": {},
   "source": [
    "## 14. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_loss, val_accuracy = model.evaluate(val_inputs, val_labels, verbose=0)\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f532217",
   "metadata": {},
   "source": [
    "## 15. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_labels, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f157d",
   "metadata": {},
   "source": [
    "## 16. Tune Decision Threshold for Optimal F1 Score\n",
    "\n",
    "Instead of using default 0.5 threshold, we'll find the optimal threshold that maximizes F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba53e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on validation set to tune threshold\n",
    "print(\"Tuning decision threshold on validation set...\")\n",
    "val_predictions = model.predict(val_inputs, verbose=0)\n",
    "val_probabilities = tf.nn.softmax(val_predictions.logits)[:, 1].numpy()\n",
    "\n",
    "# Try different thresholds and find the one that maximizes F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "thresholds_to_try = np.arange(0.3, 0.7, 0.01)\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "for threshold in thresholds_to_try:\n",
    "    val_preds_at_threshold = (val_probabilities >= threshold).astype(int)\n",
    "    f1 = f1_score(val_labels, val_preds_at_threshold)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nOptimal threshold: {best_threshold:.3f}\")\n",
    "print(f\"F1 Score at optimal threshold: {best_f1:.4f}\")\n",
    "print(f\"F1 Score at default 0.5: {f1_score(val_labels, (val_probabilities >= 0.5).astype(int)):.4f}\")\n",
    "print(f\"\\nUsing threshold {best_threshold:.3f} for test set evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0f318",
   "metadata": {},
   "source": [
    "## 16. Plot Training and Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b171b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s')\n",
    "plt.title('XLM-RoBERTa: Training and Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2, marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2, marker='s')\n",
    "plt.title('XLM-RoBERTa: Training and Validation Loss', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/xlm_roberta_training_history.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a825a0",
   "metadata": {},
   "source": [
    "## 17. Plot Training and Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677abbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Predict labels for the test set\n",
    "print(\"Generating predictions on test set...\")\n",
    "predictions = model.predict(test_inputs)\n",
    "predicted_labels = np.argmax(predictions.logits, axis=1)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "# Display confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['HUMAN', 'AI'])\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title('Confusion Matrix - XLM-RoBERTa Large Text Classification', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/xlm_roberta_confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved!\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s')\n",
    "plt.title('XLM-RoBERTa Large: Training and Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2, marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2, marker='s')\n",
    "plt.title('XLM-RoBERTa Large: Training and Validation Loss', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/xlm_roberta_large_training_history.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c4154",
   "metadata": {},
   "source": [
    "## 18. Generate Predictions with Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities for positive class (AI)\n",
    "probabilities = tf.nn.softmax(predictions.logits)[:, 1]\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_score = roc_auc_score(test_labels, probabilities)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, probabilities)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'XLM-RoBERTa (AUC = {auc_score:.4f})', linewidth=2, color='darkgreen')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=11)\n",
    "plt.ylabel('True Positive Rate', fontsize=11)\n",
    "plt.title('ROC Curve - XLM-RoBERTa Text Classification', fontsize=12, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/xlm_roberta_roc_curve.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Predict labels for the test set using optimal threshold\n",
    "print(\"Generating predictions on test set with optimal threshold...\")\n",
    "test_predictions = model.predict(test_inputs, verbose=0)\n",
    "test_probabilities = tf.nn.softmax(test_predictions.logits)[:, 1].numpy()\n",
    "\n",
    "# Use optimal threshold instead of default 0.5\n",
    "predicted_labels = (test_probabilities >= best_threshold).astype(int)\n",
    "\n",
    "print(f\"Using threshold: {best_threshold:.3f}\")\n",
    "print(f\"Predictions generated!\\n\")\n",
    "\n",
    "# Also get predictions at default threshold for comparison\n",
    "predicted_labels_default = np.argmax(test_predictions.logits, axis=1)\n",
    "\n",
    "# Generate confusion matrices\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "cm_default = confusion_matrix(test_labels, predicted_labels_default)\n",
    "\n",
    "# Display confusion matrices side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm_default, display_labels=['HUMAN', 'AI'])\n",
    "disp1.plot(cmap=plt.cm.Blues, values_format='d', ax=ax1)\n",
    "ax1.set_title('Confusion Matrix - Default Threshold (0.5)', fontsize=12, fontweight='bold')\n",
    "\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['HUMAN', 'AI'])\n",
    "disp2.plot(cmap=plt.cm.Greens, values_format='d', ax=ax2)\n",
    "ax2.set_title(f'Confusion Matrix - Optimal Threshold ({best_threshold:.3f})', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/xlm_roberta_large_confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrices saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0175ffa",
   "metadata": {},
   "source": [
    "## 19. Calculate ROC Curve and AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ee436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification reports for both thresholds\n",
    "report_optimal = classification_report(\n",
    "    test_labels,\n",
    "    predicted_labels,\n",
    "    target_names=['HUMAN', 'AI'],\n",
    "    digits=4\n",
    ")\n",
    "\n",
    "report_default = classification_report(\n",
    "    test_labels,\n",
    "    predicted_labels_default,\n",
    "    target_names=['HUMAN', 'AI'],\n",
    "    digits=4\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION REPORT - DEFAULT THRESHOLD (0.5)\")\n",
    "print(\"=\"*70)\n",
    "print(report_default)\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"CLASSIFICATION REPORT - OPTIMAL THRESHOLD ({best_threshold:.3f})\")\n",
    "print(\"=\"*70)\n",
    "print(report_optimal)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate per-class metrics for comparison\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision_opt, recall_opt, f1_opt, _ = precision_recall_fscore_support(\n",
    "    test_labels, predicted_labels, average=None, labels=[0, 1]\n",
    ")\n",
    "\n",
    "precision_def, recall_def, f1_def, _ = precision_recall_fscore_support(\n",
    "    test_labels, predicted_labels_default, average=None, labels=[0, 1]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY METRICS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<30} {'Default (0.5)':<20} {'Optimal':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'AI Recall (Most Important)':<30} {recall_def[1]:<20.4f} {recall_opt[1]:<20.4f}\")\n",
    "print(f\"{'AI Precision':<30} {precision_def[1]:<20.4f} {precision_opt[1]:<20.4f}\")\n",
    "print(f\"{'AI F1-Score':<30} {f1_def[1]:<20.4f} {f1_opt[1]:<20.4f}\")\n",
    "print(f\"{'Human Recall':<30} {recall_def[0]:<20.4f} {recall_opt[0]:<20.4f}\")\n",
    "print(f\"{'Human Precision':<30} {precision_def[0]:<20.4f} {precision_opt[0]:<20.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_score = roc_auc_score(test_labels, test_probabilities)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, test_probabilities)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'XLM-RoBERTa Large (AUC = {auc_score:.4f})', linewidth=2, color='darkgreen')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier')\n",
    "\n",
    "# Mark the optimal threshold point on ROC curve\n",
    "optimal_idx = np.argmin(np.abs(thresholds - best_threshold))\n",
    "plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100, zorder=5, \n",
    "            label=f'Optimal Threshold ({best_threshold:.3f})')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=11)\n",
    "plt.ylabel('True Positive Rate', fontsize=11)\n",
    "plt.title('ROC Curve - XLM-RoBERTa Large', fontsize=12, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/xlm_roberta_large_roc_curve.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ae65f",
   "metadata": {},
   "source": [
    "## 21. Summary of Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81697af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracies at both thresholds\n",
    "accuracy_optimal = accuracy_score(test_labels, predicted_labels)\n",
    "accuracy_default = accuracy_score(test_labels, predicted_labels_default)\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Model',\n",
    "        'Parameters',\n",
    "        'Model Size',\n",
    "        'Max Sequence Length',\n",
    "        'Training Epochs',\n",
    "        'Learning Rate',\n",
    "        'Effective Batch Size',\n",
    "        '',\n",
    "        'Test Accuracy (Default 0.5)',\n",
    "        'Test Accuracy (Optimal Threshold)',\n",
    "        'Test Loss',\n",
    "        'AUC Score',\n",
    "        'Optimal Threshold',\n",
    "        '',\n",
    "        'AI Recall (Default)',\n",
    "        'AI Recall (Optimal)',\n",
    "        'AI Precision (Optimal)',\n",
    "        'AI F1-Score (Optimal)',\n",
    "        '',\n",
    "        'Human Recall (Optimal)',\n",
    "        'Human Precision (Optimal)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        'XLM-RoBERTa Large',\n",
    "        f'{model.num_parameters():,}',\n",
    "        '~2.2GB',\n",
    "        '256 tokens',\n",
    "        f'{len(history.history[\"loss\"])}',\n",
    "        '1.5e-5',\n",
    "        '32 (16 x 2 accumulation)',\n",
    "        '',\n",
    "        f'{accuracy_default:.4f}',\n",
    "        f'{accuracy_optimal:.4f}',\n",
    "        f'{test_loss:.4f}',\n",
    "        f'{auc_score:.4f}',\n",
    "        f'{best_threshold:.3f}',\n",
    "        '',\n",
    "        f'{recall_def[1]:.4f}',\n",
    "        f'{recall_opt[1]:.4f}',\n",
    "        f'{precision_opt[1]:.4f}',\n",
    "        f'{f1_opt[1]:.4f}',\n",
    "        '',\n",
    "        f'{recall_opt[0]:.4f}',\n",
    "        f'{precision_opt[0]:.4f}'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"XLM-RoBERTa LARGE MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_df.to_csv('results/xlm_roberta_large_performance_summary.csv', index=False)\n",
    "print(\"\\nPerformance summary saved to results/xlm_roberta_large_performance_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b8b119",
   "metadata": {},
   "source": [
    "## 22. Test the Model with Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc45e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, model, tokenizer, threshold=best_threshold):\n",
    "    \"\"\"\n",
    "    Predict whether a given text is AI-generated or Human-written\n",
    "    Uses the optimal threshold for classification\n",
    "    \"\"\"\n",
    "    # Preprocess the text\n",
    "    processed_text = expand_contractions(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        processed_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict({\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask']\n",
    "    }, verbose=0)\n",
    "    \n",
    "    # Get probabilities\n",
    "    probs = tf.nn.softmax(predictions.logits)[0]\n",
    "    ai_prob = float(probs[1])\n",
    "    \n",
    "    # Use optimal threshold\n",
    "    predicted_label = 1 if ai_prob >= threshold else 0\n",
    "    \n",
    "    return {\n",
    "        'label': reverse_mapping[predicted_label],\n",
    "        'human_prob': float(probs[0]),\n",
    "        'ai_prob': ai_prob,\n",
    "        'confidence': ai_prob if predicted_label == 1 else float(probs[0]),\n",
    "        'threshold_used': threshold\n",
    "    }\n",
    "\n",
    "# Test with sample texts from test set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"SAMPLE PREDICTIONS (Using Optimal Threshold: {best_threshold:.3f})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_indices = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "for idx in sample_indices:\n",
    "    if idx < len(test_df):\n",
    "        text = test_df.iloc[idx]['text']\n",
    "        true_label = test_df.iloc[idx]['label']\n",
    "        \n",
    "        result = predict_text(text, model, tokenizer)\n",
    "        \n",
    "        correct = \"✓\" if result['label'] == true_label else \"✗\"\n",
    "        \n",
    "        print(f\"\\nText: {text[:100]}...\")\n",
    "        print(f\"True Label: {true_label}\")\n",
    "        print(f\"Predicted: {result['label']} {correct} (Confidence: {result['confidence']:.2%})\")\n",
    "        print(f\"Human Prob: {result['human_prob']:.2%} | AI Prob: {result['ai_prob']:.2%}\")\n",
    "        print(\"-\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
