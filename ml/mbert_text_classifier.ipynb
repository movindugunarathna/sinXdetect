{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5aef3e",
   "metadata": {},
   "source": [
    "# Multilingual BERT (mBERT) Text Classifier for Sinhala AI Detection\n",
    "\n",
    "This notebook implements a text classification model using Google's mBERT (bert-base-multilingual-cased) model to detect AI-generated Sinhala text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f5e65",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tf-keras\n",
    "%pip install -q transformers\n",
    "%pip install -q datasets\n",
    "%pip install -q nltk\n",
    "%pip install -q scikit-learn\n",
    "%pip install -q matplotlib\n",
    "%pip install -q seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe89ff3",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve, \n",
    "    roc_auc_score, \n",
    "    classification_report,\n",
    "    precision_recall_curve,\n",
    "    f1_score\n",
    ")\n",
    "import re\n",
    "import unicodedata\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    TFBertForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a72b530",
   "metadata": {},
   "source": [
    "## 3. Load Dataset from JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721f8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load data from JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Load training, validation, and test datasets\n",
    "train_data = load_jsonl('dataset/train.jsonl')\n",
    "val_data = load_jsonl('dataset/val.jsonl')\n",
    "test_data = load_jsonl('dataset/test.jsonl')\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Testing set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f21e5",
   "metadata": {},
   "source": [
    "## 4. Convert to DataFrame and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778daac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Display sample data\n",
    "print(\"Sample training data:\")\n",
    "print(train_df[['text', 'label']].head())\n",
    "print(f\"\\nLabel distribution (Train):\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(f\"\\nLabel distribution (Validation):\")\n",
    "print(val_df['label'].value_counts())\n",
    "print(f\"\\nLabel distribution (Test):\")\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e62cc",
   "metadata": {},
   "source": [
    "## 5. Map Labels to Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b6dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mapping\n",
    "label_mapping = {'HUMAN': 0, 'AI': 1}\n",
    "reverse_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Map labels to numeric values\n",
    "train_df['label_encoded'] = train_df['label'].map(label_mapping)\n",
    "val_df['label_encoded'] = val_df['label'].map(label_mapping)\n",
    "test_df['label_encoded'] = test_df['label'].map(label_mapping)\n",
    "\n",
    "# Check for any unmapped values\n",
    "print(f\"Train - Unmapped labels: {train_df['label_encoded'].isna().sum()}\")\n",
    "print(f\"Val - Unmapped labels: {val_df['label_encoded'].isna().sum()}\")\n",
    "print(f\"Test - Unmapped labels: {test_df['label_encoded'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9ff6d",
   "metadata": {},
   "source": [
    "## 6. Sinhala Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea771881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sinhala_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive Sinhala text preprocessing:\n",
    "    - NFC Unicode normalization\n",
    "    - Remove Zero-Width characters\n",
    "    - Expand common contractions\n",
    "    - Clean extra whitespace\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Normalize to NFC (Canonical Composition)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Remove zero-width characters (ZWNJ/ZWJ)\n",
    "    text = text.replace('\\u200c', '').replace('\\u200d', '')\n",
    "    \n",
    "    # Sinhala contractions and colloquial forms\n",
    "    contractions_si = {\n",
    "        # Negation forms\n",
    "        'නෑ': 'නැහැ',\n",
    "        'බෑ': 'බැහැ',\n",
    "        'හොයන්නෑ': 'හොයන්න නැහැ',\n",
    "        # Common colloquial forms\n",
    "        'දැං': 'දැන්',\n",
    "        'කොහේද': 'කොහෙද',\n",
    "        'මොකෝ': 'මොකද',\n",
    "        'එහෙනං': 'එහෙනම්',\n",
    "        # Location/place markers\n",
    "        'ඇතුලෙ': 'ඇතුලේ',\n",
    "        'බාහිරෙ': 'බාහිරේ',\n",
    "        'වැඩෙ': 'වැඩේ',\n",
    "        'රජයෙ': 'රජයේ',\n",
    "        'යාලුවෙ': 'යාලුවේ',\n",
    "    }\n",
    "    \n",
    "    # Apply word-boundary replacements\n",
    "    for contraction, expanded in contractions_si.items():\n",
    "        text = re.sub(r\"\\b\" + re.escape(contraction) + r\"\\b\", expanded, text)\n",
    "    \n",
    "    # Targeted possessive expansions\n",
    "    possessive_map = {\n",
    "        r\"\\bමගෙ\\b\": \"මගේ\",\n",
    "        r\"\\bඔයාගෙ\\b\": \"ඔයාගේ\",\n",
    "        r\"\\bඔගෙ\\b\": \"ඔගේ\",\n",
    "        r\"\\bඑයාගෙ\\b\": \"එයාගේ\",\n",
    "    }\n",
    "    \n",
    "    for pattern, replacement in possessive_map.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    \n",
    "    # Clean extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing training data...\")\n",
    "train_df['processed_text'] = train_df['text'].apply(preprocess_sinhala_text)\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_df['processed_text'] = val_df['text'].apply(preprocess_sinhala_text)\n",
    "\n",
    "print(\"Preprocessing test data...\")\n",
    "test_df['processed_text'] = test_df['text'].apply(preprocess_sinhala_text)\n",
    "\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "print(f\"\\nExample preprocessed text:\")\n",
    "print(f\"Original: {train_df['text'].iloc[0][:100]}...\")\n",
    "print(f\"Processed: {train_df['processed_text'].iloc[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19dba3",
   "metadata": {},
   "source": [
    "## 7. Load mBERT Tokenizer\n",
    "\n",
    "We use `bert-base-multilingual-cased` which supports 104 languages including Sinhala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d61f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mBERT tokenizer (multilingual cased version)\n",
    "MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"✓ mBERT tokenizer loaded: {MODEL_NAME}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Max length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494eae15",
   "metadata": {},
   "source": [
    "## 8. Tokenize and Encode Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542676cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_LENGTH = 256  # Optimal for most Sinhala text\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def tokenize_in_batches(texts, tokenizer, batch_size=32, max_length=256):\n",
    "    \"\"\"\n",
    "    Tokenize texts in batches to manage memory efficiently.\n",
    "    Returns dict with 'input_ids' and 'attention_mask' as numpy arrays.\n",
    "    \"\"\"\n",
    "    input_ids_parts = []\n",
    "    attention_mask_parts = []\n",
    "    \n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        \n",
    "        # Convert to numpy\n",
    "        ids = enc['input_ids'].numpy()\n",
    "        mask = enc['attention_mask'].numpy()\n",
    "        \n",
    "        input_ids_parts.append(ids)\n",
    "        attention_mask_parts.append(mask)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Processed {i // batch_size + 1}/{total_batches} batches\")\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    input_ids = np.concatenate(input_ids_parts, axis=0)\n",
    "    attention_mask = np.concatenate(attention_mask_parts, axis=0)\n",
    "    \n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing training data...\")\n",
    "train_encodings = tokenize_in_batches(\n",
    "    train_df['processed_text'].tolist(), \n",
    "    tokenizer, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "print(\"\\nTokenizing validation data...\")\n",
    "val_encodings = tokenize_in_batches(\n",
    "    val_df['processed_text'].tolist(), \n",
    "    tokenizer, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "print(\"\\nTokenizing test data...\")\n",
    "test_encodings = tokenize_in_batches(\n",
    "    test_df['processed_text'].tolist(), \n",
    "    tokenizer, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Tokenization complete!\")\n",
    "print(f\"Training encodings shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"Validation encodings shape: {val_encodings['input_ids'].shape}\")\n",
    "print(f\"Test encodings shape: {test_encodings['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde7757c",
   "metadata": {},
   "source": [
    "## 9. Prepare Input Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input dictionaries\n",
    "train_inputs = {\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask']\n",
    "}\n",
    "\n",
    "val_inputs = {\n",
    "    'input_ids': val_encodings['input_ids'],\n",
    "    'attention_mask': val_encodings['attention_mask']\n",
    "}\n",
    "\n",
    "test_inputs = {\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask']\n",
    "}\n",
    "\n",
    "# Prepare labels\n",
    "train_labels = np.array(train_df['label_encoded'].astype(int).tolist())\n",
    "val_labels = np.array(val_df['label_encoded'].astype(int).tolist())\n",
    "test_labels = np.array(test_df['label_encoded'].astype(int).tolist())\n",
    "\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Val labels shape: {val_labels.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "print(f\"\\nLabel distribution (Train): {np.bincount(train_labels)}\")\n",
    "print(f\"Label distribution (Val): {np.bincount(val_labels)}\")\n",
    "print(f\"Label distribution (Test): {np.bincount(test_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c59d0",
   "metadata": {},
   "source": [
    "## 10. Load mBERT Model for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained mBERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,  # Binary classification: HUMAN (0) vs AI (1)\n",
    "    from_pt=True   # Convert from PyTorch if needed\n",
    ")\n",
    "\n",
    "print(f\"✓ mBERT model loaded: {MODEL_NAME}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f614d",
   "metadata": {},
   "source": [
    "## 11. Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b31ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[metric]\n",
    ")\n",
    "\n",
    "print(\"✓ Model compiled successfully!\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"  Max Length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6ef2e",
   "metadata": {},
   "source": [
    "## 12. Set Up Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ceaf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks for training\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=2,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=1,\n",
    "        verbose=1,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"✓ Callbacks configured:\")\n",
    "print(\"  - Early Stopping (patience=2)\")\n",
    "print(\"  - Learning Rate Reduction (factor=0.5, patience=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d459b",
   "metadata": {},
   "source": [
    "## 13. Train the mBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Training...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    validation_data=(val_inputs, val_labels),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Training Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e2a71",
   "metadata": {},
   "source": [
    "## 14. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory if it doesn't exist\n",
    "MODEL_SAVE_PATH = \"models/mbert_sinhala_classifier/\"\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(MODEL_SAVE_PATH)\n",
    "print(f\"✓ Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "print(f\"✓ Tokenizer saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Save training configuration\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'batch_size': TRAIN_BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'epochs': EPOCHS,\n",
    "    'label_mapping': label_mapping\n",
    "}\n",
    "\n",
    "with open(os.path.join(MODEL_SAVE_PATH, 'training_config.json'), 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"✓ Configuration saved to {MODEL_SAVE_PATH}training_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4774d",
   "metadata": {},
   "source": [
    "## 15. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"\\nEvaluating on Validation Set...\")\n",
    "val_loss, val_accuracy = model.evaluate(val_inputs, val_labels, batch_size=32, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALIDATION SET RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Loss: {val_loss:.4f}\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c191838",
   "metadata": {},
   "source": [
    "## 16. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2537e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on Test Set...\")\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_labels, batch_size=32, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Loss: {test_loss:.4f}\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21436d",
   "metadata": {},
   "source": [
    "## 17. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2.5, marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2.5, marker='s')\n",
    "plt.title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2.5, marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2.5, marker='s')\n",
    "plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/mbert_training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training history plot saved to results/mbert_training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccd39d",
   "metadata": {},
   "source": [
    "## 18. Generate Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f29eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "print(\"Generating predictions on test set...\")\n",
    "predictions = model.predict(test_inputs, batch_size=32, verbose=0)\n",
    "\n",
    "# Get predicted labels\n",
    "predicted_labels = np.argmax(predictions.logits, axis=1)\n",
    "\n",
    "# Get prediction probabilities\n",
    "probabilities = tf.nn.softmax(predictions.logits).numpy()\n",
    "positive_class_probs = probabilities[:, 1]  # Probability for AI class\n",
    "\n",
    "print(f\"✓ Predictions generated for {len(predicted_labels)} samples\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(f\"  HUMAN: {np.sum(predicted_labels == 0)}\")\n",
    "print(f\"  AI: {np.sum(predicted_labels == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a2d616",
   "metadata": {},
   "source": [
    "## 19. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11406ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm, \n",
    "    display_labels=['HUMAN', 'AI']\n",
    ")\n",
    "disp.plot(cmap='Blues', values_format='d', ax=plt.gca())\n",
    "\n",
    "plt.title('Confusion Matrix - mBERT Sinhala Classifier', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/mbert_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrix saved to results/mbert_confusion_matrix.png\")\n",
    "\n",
    "# Print confusion matrix values\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"True Negatives (HUMAN->HUMAN): {cm[0, 0]}\")\n",
    "print(f\"False Positives (HUMAN->AI): {cm[0, 1]}\")\n",
    "print(f\"False Negatives (AI->HUMAN): {cm[1, 0]}\")\n",
    "print(f\"True Positives (AI->AI): {cm[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3bdd3f",
   "metadata": {},
   "source": [
    "## 20. ROC Curve and AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ec045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, positive_class_probs)\n",
    "auc_score = roc_auc_score(test_labels, positive_class_probs)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, label=f'mBERT (AUC = {auc_score:.4f})', linewidth=3, color='#2E86DE')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', linewidth=2, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - mBERT Sinhala Classifier', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/mbert_roc_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ ROC curve saved to results/mbert_roc_curve.png\")\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aade91a1",
   "metadata": {},
   "source": [
    "## 21. Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5568e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision-recall curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "precision, recall, pr_thresholds = precision_recall_curve(test_labels, positive_class_probs)\n",
    "avg_precision = average_precision_score(test_labels, positive_class_probs)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall, precision, linewidth=3, color='#10AC84', label=f'mBERT (AP = {avg_precision:.4f})')\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve - mBERT Sinhala Classifier', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.legend(loc='lower left', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/mbert_precision_recall_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Precision-Recall curve saved to results/mbert_precision_recall_curve.png\")\n",
    "print(f\"\\nAverage Precision Score: {avg_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea7349",
   "metadata": {},
   "source": [
    "## 22. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(\n",
    "    test_labels,\n",
    "    predicted_labels,\n",
    "    target_names=['HUMAN', 'AI'],\n",
    "    digits=4\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION REPORT - TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(report)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save report to file\n",
    "with open('results/mbert_classification_report.txt', 'w') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"CLASSIFICATION REPORT - mBERT Sinhala Classifier\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(report)\n",
    "    f.write(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\n✓ Classification report saved to results/mbert_classification_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c81e5",
   "metadata": {},
   "source": [
    "## 23. Detailed Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f30049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "precision = precision_score(test_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(test_labels, predicted_labels, average='weighted')\n",
    "f1 = f1_score(test_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Class-specific metrics\n",
    "precision_per_class = precision_score(test_labels, predicted_labels, average=None)\n",
    "recall_per_class = recall_score(test_labels, predicted_labels, average=None)\n",
    "f1_per_class = f1_score(test_labels, predicted_labels, average=None)\n",
    "\n",
    "# Create summary DataFrame\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Test Accuracy',\n",
    "        'Test Loss',\n",
    "        'AUC Score',\n",
    "        'Average Precision',\n",
    "        'Weighted Precision',\n",
    "        'Weighted Recall',\n",
    "        'Weighted F1-Score',\n",
    "        'HUMAN - Precision',\n",
    "        'HUMAN - Recall',\n",
    "        'HUMAN - F1-Score',\n",
    "        'AI - Precision',\n",
    "        'AI - Recall',\n",
    "        'AI - F1-Score'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f'{accuracy:.4f}',\n",
    "        f'{test_loss:.4f}',\n",
    "        f'{auc_score:.4f}',\n",
    "        f'{avg_precision:.4f}',\n",
    "        f'{precision:.4f}',\n",
    "        f'{recall:.4f}',\n",
    "        f'{f1:.4f}',\n",
    "        f'{precision_per_class[0]:.4f}',\n",
    "        f'{recall_per_class[0]:.4f}',\n",
    "        f'{f1_per_class[0]:.4f}',\n",
    "        f'{precision_per_class[1]:.4f}',\n",
    "        f'{recall_per_class[1]:.4f}',\n",
    "        f'{f1_per_class[1]:.4f}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(metrics_summary.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save metrics\n",
    "metrics_summary.to_csv('results/mbert_performance_metrics.csv', index=False)\n",
    "print(\"\\n✓ Metrics saved to results/mbert_performance_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d042d403",
   "metadata": {},
   "source": [
    "## 24. Test Model on Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b93edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text, model, tokenizer, max_length=256):\n",
    "    \"\"\"\n",
    "    Predict if a given text is HUMAN or AI-generated.\n",
    "    Returns prediction and confidence score.\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    processed_text = preprocess_sinhala_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer(\n",
    "        [processed_text],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    outputs = model(encoding)\n",
    "    probs = tf.nn.softmax(outputs.logits, axis=1).numpy()[0]\n",
    "    \n",
    "    predicted_label = np.argmax(probs)\n",
    "    confidence = probs[predicted_label]\n",
    "    \n",
    "    return {\n",
    "        'label': reverse_mapping[predicted_label],\n",
    "        'confidence': float(confidence),\n",
    "        'human_prob': float(probs[0]),\n",
    "        'ai_prob': float(probs[1])\n",
    "    }\n",
    "\n",
    "# Test on sample texts from test set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_indices = np.random.choice(len(test_df), 5, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    text = test_df.iloc[idx]['text']\n",
    "    true_label = test_df.iloc[idx]['label']\n",
    "    \n",
    "    result = predict_text(text, model, tokenizer, MAX_LENGTH)\n",
    "    \n",
    "    print(f\"\\nText: {text[:100]}...\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted: {result['label']} (Confidence: {result['confidence']:.2%})\")\n",
    "    print(f\"  HUMAN: {result['human_prob']:.2%} | AI: {result['ai_prob']:.2%}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ad803",
   "metadata": {},
   "source": [
    "## 25. Model Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"mBERT SINHALA CLASSIFIER - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")\n",
    "print(f\"Max Sequence Length: {MAX_LENGTH}\")\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"  Training: {len(train_df):,} samples\")\n",
    "print(f\"  Validation: {len(val_df):,} samples\")\n",
    "print(f\"  Test: {len(test_df):,} samples\")\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  AUC Score: {auc_score:.4f}\")\n",
    "print(f\"  Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "print(f\"\\nModel saved to: {MODEL_SAVE_PATH}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✓ All tasks completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
